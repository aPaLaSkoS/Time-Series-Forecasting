{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyNwA7nTa4gfcqvAvft7XlMw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"be232ecaf7e94be986d8b58b38bbdf8a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f385ac54f20646d085610599c4c04290","IPY_MODEL_56cf893ef72d4233979d45b3942b7bae","IPY_MODEL_e444191b66c44ddc9984967d15027d03"],"layout":"IPY_MODEL_7f67dc351c4a434fa71737593b4ea9e5"}},"f385ac54f20646d085610599c4c04290":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb6511746e7540f3a3baf99f1e89241a","placeholder":"​","style":"IPY_MODEL_c97d144f6a35435fadc5734b1505ddbf","value":" 31%"}},"56cf893ef72d4233979d45b3942b7bae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_089cf9477a654829a5ba3717c8bad709","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ecbd0457f2943cca74e9dfe3bd8ad0c","value":312}},"e444191b66c44ddc9984967d15027d03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1273668c5c8c4554b7b520d9845600a4","placeholder":"​","style":"IPY_MODEL_4644b636fc25463cafc1cb2a386ad874","value":" 312/1000 [28:37&lt;1:03:04,  5.50s/it]"}},"7f67dc351c4a434fa71737593b4ea9e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb6511746e7540f3a3baf99f1e89241a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c97d144f6a35435fadc5734b1505ddbf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"089cf9477a654829a5ba3717c8bad709":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ecbd0457f2943cca74e9dfe3bd8ad0c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1273668c5c8c4554b7b520d9845600a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4644b636fc25463cafc1cb2a386ad874":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Installs + Imports"],"metadata":{"id":"-ab2qm_WcYTh"}},{"cell_type":"code","source":["!pip install pytorch_lightning\n","!pip install torchmetrics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LaQeZBCW-1Sh","executionInfo":{"status":"ok","timestamp":1684950091207,"user_tz":-180,"elapsed":14748,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}},"outputId":"a09c9e79-921f-4a48-b6aa-d8f94720bccc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_lightning\n","  Downloading pytorch_lightning-2.0.2-py3-none-any.whl (719 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.0/719.0 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.22.4)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.0.1+cu118)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.65.0)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0)\n","Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.4.0)\n","Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (23.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.5.0)\n","Collecting lightning-utilities>=0.7.0 (from pytorch_lightning)\n","  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.27.1)\n","Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>2021.06.0->pytorch_lightning)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.12.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (16.0.5)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch_lightning) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch_lightning) (1.3.0)\n","Installing collected packages: multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch_lightning\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch_lightning-2.0.2 torchmetrics-0.11.4 yarl-1.9.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (0.11.4)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"]}]},{"cell_type":"code","source":["import math\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pytorch_lightning as pl\n","\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","from google.colab import files\n","from tqdm.auto import tqdm\n","from torchmetrics import MeanAbsolutePercentageError\n","from datetime import datetime \n","from typing import Tuple\n","from functools import partial\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from math import sqrt"],"metadata":{"id":"hrEAsoZx_B8n","executionInfo":{"status":"ok","timestamp":1684950096597,"user_tz":-180,"elapsed":5396,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Classes + Helpers"],"metadata":{"id":"msgcmWKKKmJS"}},{"cell_type":"markdown","source":["## Data processing"],"metadata":{"id":"5VrJ6nvMbK8w"}},{"cell_type":"code","source":["def scale_data(load_df, \n","               start_train_date,\n","               end_val_date,\n","               start_test_date,\n","               end_test_date):\n","  \n","  train_val_df = load_df[(load_df.index >= start_train_date) &\n","                        (load_df.index <= end_val_date)]\n","  test_df = load_df[(load_df.index >= start_test_date) &\n","                    (load_df.index <= end_test_date)]\n","\n","  scaler = MaxAbsScaler()   # MinMaxScaler()\n","  train_val_scaled = scaler.fit_transform(train_val_df)\n","  train_val_df_scaled = pd.DataFrame(train_val_scaled,\n","                                    columns=train_val_df.columns,\n","                                    index=train_val_df.index)\n","  test_scaled = scaler.transform(test_df)\n","  test_df_scaled = pd.DataFrame(test_scaled,\n","                                columns=test_df.columns,\n","                                index=test_df.index)\n","\n","  load_df_scaled = pd.concat([train_val_df_scaled, test_df_scaled], axis=0)\n","\n","  return load_df_scaled, scaler"],"metadata":{"id":"20DCtZ_EJ8ti","executionInfo":{"status":"ok","timestamp":1684950096598,"user_tz":-180,"elapsed":13,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def reframing_enc(X_df, skip_steps_back, n_backwards):\n","\n","  feat_cols, feat_names = [], []\n","  # iterate through all columns\n","  for col_index, col_name in enumerate(X_df.columns):\n","    series = X_df[col_name].copy()\n","    # input sequence (t-skip_steps_back, ... ,t-(n_backwards+1) )\n","    for b in range(skip_steps_back, n_backwards):\n","      feat_cols.append(series.shift(b))\n","      feat_names.append(f'{col_name}_(t-{b})')\n","  \n","  # put it all together\n","  X = pd.concat(feat_cols, axis=1)\n","  X.columns = feat_names\n","  # drop rows with NaN values\n","  X.dropna(inplace=True)\n","  \n","  return X\n","\n","def reframe_enc_data(load_df_scaled,\n","                     target,\n","                     skip_steps_back,\n","                     last_step_back,\n","                     last_step_forward):\n","\n","  # shift future values\n","  for col in load_df_scaled.drop(target, axis=1).columns:\n","    load_df_scaled[col + f'_(t+{last_step_forward})'] = load_df_scaled[col].shift(-last_step_forward)\n","    load_df_scaled.drop(col, axis=1, inplace=True)\n","\n","  return reframing_enc(load_df_scaled,\n","                       skip_steps_back=skip_steps_back, \n","                       n_backwards=last_step_back)\n","\n","def split_enc_data(load_df_scaled_reframed,\n","                    start_train_date,\n","                    end_train_date,\n","                    start_val_date,\n","                    end_val_date,\n","                    start_test_date,\n","                    end_test_date,\n","                    last_step_back):\n","\n","  load_train_df_scaled_reframed = load_df_scaled_reframed[(load_df_scaled_reframed.index >= start_train_date) & \n","                                                          (load_df_scaled_reframed.index <= end_train_date)]\n","\n","  load_val_df_scaled_reframed = load_df_scaled_reframed[(load_df_scaled_reframed.index >= start_val_date) & \n","                                                        (load_df_scaled_reframed.index <= end_val_date)]\n","\n","  load_test_df_scaled_reframed = load_df_scaled_reframed[(load_df_scaled_reframed.index >= start_test_date) & \n","                                                        (load_df_scaled_reframed.index <= end_test_date)]\n","\n","  X_train_3D = create3Dinput(load_train_df_scaled_reframed, last_step_back)\n","  X_val_3D = create3Dinput(load_val_df_scaled_reframed, last_step_back)\n","  X_test_3D = create3Dinput(load_test_df_scaled_reframed, last_step_back)\n","\n","  return X_train_3D, X_val_3D, X_test_3D"],"metadata":{"id":"Gg8fgu2er-ie","executionInfo":{"status":"ok","timestamp":1684950096598,"user_tz":-180,"elapsed":12,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def reframing_dec(X_df, n_backwards):\n","\n","  feat_cols, feat_names = [], []\n","  # iterate through all columns\n","  for col_index, col_name in enumerate(X_df.columns):\n","    series = X_df[col_name].copy()\n","    # input sequence (t, t-1, ... ,t-(n_backwards+1) )\n","    for b in range(n_backwards):\n","      feat_cols.append(series.shift(b))\n","      feat_names.append(f'{col_name}_(t-{b})')\n","  \n","  # put it all together\n","  X = pd.concat(feat_cols, axis=1)\n","  X.columns = feat_names\n","  # drop rows with NaN values\n","  X.dropna(inplace=True)\n","  \n","  return X\n","\n","def reframe_dec_data(load_df_scaled,\n","                     last_step_back,\n","                     last_step_forward):\n","\n","  # shift future values (weather + time data)\n","  for col in load_df_scaled.columns:\n","    load_df_scaled[col + f'_(t+{last_step_forward})'] = load_df_scaled[col].shift(-last_step_forward)\n","    load_df_scaled.drop(col, axis=1, inplace=True)\n","\n","  return reframing_dec(load_df_scaled, \n","                       n_backwards=last_step_back + last_step_forward)\n","\n","def split_dec_data(load_df_scaled_reframed,\n","                    start_train_date,\n","                    end_train_date,\n","                    start_val_date,\n","                    end_val_date,\n","                    start_test_date,\n","                    end_test_date,\n","                    steps_forward,\n","                    last_step_back):\n","\n","  load_train_df_scaled_reframed = load_df_scaled_reframed[(load_df_scaled_reframed.index >= start_train_date) & \n","                                                          (load_df_scaled_reframed.index <= end_train_date)]\n","\n","  load_val_df_scaled_reframed = load_df_scaled_reframed[(load_df_scaled_reframed.index >= start_val_date) & \n","                                                        (load_df_scaled_reframed.index <= end_val_date)]\n","\n","  load_test_df_scaled_reframed = load_df_scaled_reframed[(load_df_scaled_reframed.index >= start_test_date) & \n","                                                        (load_df_scaled_reframed.index <= end_test_date)]\n","\n","  X_train_3D = create3Dinput(load_train_df_scaled_reframed, last_step_back+steps_forward)\n","  X_val_3D = create3Dinput(load_val_df_scaled_reframed, last_step_back+steps_forward)\n","  X_test_3D = create3Dinput(load_test_df_scaled_reframed, last_step_back+steps_forward)\n","\n","  return X_train_3D, X_val_3D, X_test_3D"],"metadata":{"id":"gX236rlG64-D","executionInfo":{"status":"ok","timestamp":1684950096599,"user_tz":-180,"elapsed":12,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def create3Dinput(df, steps):\n","  N, D = df.shape\n","  D = int(D/steps)\n","  arr_3d = np.zeros((N, steps, D))\n","  for i in range(D):\n","    arr_3d[:, :, i] = df.iloc[:, i*steps:(i+1)*steps].values\n","  print(arr_3d.shape)\n","  return arr_3d\n","\n","class LoadDataset(Dataset):\n","  def __init__(self, X_3D_enc, X_3D_time_enc, X_3D_dec, X_3D_time_dec, y):\n","    self.X_enc = torch.tensor(X_3D_enc, dtype=torch.float32)\n","    self.X_time_enc = torch.tensor(X_3D_time_enc, dtype=torch.float32)\n","    self.X_dec = torch.tensor(X_3D_dec, dtype=torch.float32)\n","    self.X_time_dec = torch.tensor(X_3D_time_dec, dtype=torch.float32)\n","    self.y = torch.tensor(y, dtype=torch.float32)\n","  \n","  def __len__(self):\n","    return len(self.y)\n","\n","  def __getitem__(self, idx):\n","    return self.X_enc[idx], self.X_time_enc[idx], self.X_dec[idx], self.X_time_dec[idx], self.y[idx]"],"metadata":{"id":"V-8n4sGJKgGw","executionInfo":{"status":"ok","timestamp":1684950096599,"user_tz":-180,"elapsed":11,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AIkKmlPE4dLc","executionInfo":{"status":"ok","timestamp":1684950096600,"user_tz":-180,"elapsed":12,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Informer class"],"metadata":{"id":"oT2UV0KbpVfW"}},{"cell_type":"markdown","source":["### Embeddings"],"metadata":{"id":"xYFMj70vqgB7"}},{"cell_type":"code","source":["class PositionalEmbedding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super(PositionalEmbedding, self).__init__()\n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, d_model).float()\n","        pe.require_grad = False\n","\n","        position = torch.arange(0, max_len).float().unsqueeze(1)\n","        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n","\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        return self.pe[:, :x.size(1)]\n","\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, c_in, d_model):\n","        super(TokenEmbedding, self).__init__()\n","        padding = 1 if torch.__version__>='1.5.0' else 2\n","        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model, \n","                                    kernel_size=3, padding=padding, padding_mode='circular')\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv1d):\n","                nn.init.kaiming_normal_(m.weight,mode='fan_in',nonlinearity='leaky_relu')\n","\n","    def forward(self, x):\n","        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1,2)\n","        return x\n","\n","class FixedEmbedding(nn.Module):\n","    def __init__(self, c_in, d_model):\n","        super(FixedEmbedding, self).__init__()\n","\n","        w = torch.zeros(c_in, d_model).float()\n","        w.require_grad = False\n","\n","        position = torch.arange(0, c_in).float().unsqueeze(1)\n","        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n","\n","        w[:, 0::2] = torch.sin(position * div_term)\n","        w[:, 1::2] = torch.cos(position * div_term)\n","\n","        self.emb = nn.Embedding(c_in, d_model)\n","        self.emb.weight = nn.Parameter(w, requires_grad=False)\n","\n","    def forward(self, x):\n","        return self.emb(x).detach()\n","\n","class TemporalEmbedding(nn.Module):\n","    def __init__(self, d_model, embed_type='fixed', freq='h'):\n","        super(TemporalEmbedding, self).__init__()\n","\n","        minute_size = 4; hour_size = 24\n","        weekday_size = 7; day_size = 32; month_size = 13\n","\n","        Embed = FixedEmbedding if embed_type=='fixed' else nn.Embedding\n","        if freq=='t':\n","            self.minute_embed = Embed(minute_size, d_model)\n","        self.hour_embed = Embed(hour_size, d_model)\n","        self.weekday_embed = Embed(weekday_size, d_model)\n","        self.day_embed = Embed(day_size, d_model)\n","        self.month_embed = Embed(month_size, d_model)\n","    \n","    def forward(self, x):\n","        x = x.long()\n","        \n","        minute_x = self.minute_embed(x[:,:,4]) if hasattr(self, 'minute_embed') else 0.\n","        # print(f\"minute_x --> {minute_x}\")\n","        hour_x = self.hour_embed(x[:,:,3])\n","        # print(f\"hour_x --> {hour_x.shape}\")\n","        weekday_x = self.weekday_embed(x[:,:,2])\n","        # print(f\"weekday_x --> {weekday_x.shape}\")\n","        day_x = self.day_embed(x[:,:,1])\n","        # print(f\"day_x --> {day_x.shape}\")\n","        month_x = self.month_embed(x[:,:,0])\n","        # print(f\"month_x --> {month_x.shape}\")\n","        \n","        return hour_x + weekday_x + day_x + month_x + minute_x\n","\n","class TimeFeatureEmbedding(nn.Module):\n","    def __init__(self, d_model, embed_type='timeF', freq='h'):\n","        super(TimeFeatureEmbedding, self).__init__()\n","\n","        freq_map = {'h':4, 't':5, 's':6, 'm':1, 'a':1, 'w':2, 'd':3, 'b':3}\n","        d_inp = freq_map[freq]\n","        self.embed = nn.Linear(d_inp, d_model)\n","    \n","    def forward(self, x):\n","        return self.embed(x)\n","\n","class DataEmbedding(nn.Module):\n","    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n","        super(DataEmbedding, self).__init__()\n","\n","        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n","        self.position_embedding = PositionalEmbedding(d_model=d_model)\n","        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, freq=freq) if embed_type!='timeF' else TimeFeatureEmbedding(d_model=d_model, embed_type=embed_type, freq=freq)\n","\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, x, x_mark):\n","        x = self.value_embedding(x) + self.position_embedding(x) + self.temporal_embedding(x_mark)\n","        \n","        return self.dropout(x)"],"metadata":{"id":"7JgJwHuBqkoZ","executionInfo":{"status":"ok","timestamp":1684950096600,"user_tz":-180,"elapsed":12,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Masking"],"metadata":{"id":"ILZm7f8Gp5yy"}},{"cell_type":"code","source":["class TriangularCausalMask():\n","    def __init__(self, B, L, device=\"cpu\"):\n","        mask_shape = [B, 1, L, L]\n","        with torch.no_grad():\n","            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n","\n","    @property\n","    def mask(self):\n","        return self._mask\n","\n","class ProbMask():\n","    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n","        _mask = torch.ones(L, scores.shape[-1], dtype=torch.bool).to(device).triu(1)\n","        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n","        indicator = _mask_ex[torch.arange(B)[:, None, None],\n","                             torch.arange(H)[None, :, None],\n","                             index, :].to(device)\n","        self._mask = indicator.view(scores.shape).to(device)\n","    \n","    @property\n","    def mask(self):\n","        return self._mask"],"metadata":{"id":"81v845Ebp9D5","executionInfo":{"status":"ok","timestamp":1684950096600,"user_tz":-180,"elapsed":11,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Attention"],"metadata":{"id":"jFsy8i1apr-7"}},{"cell_type":"code","source":["class FullAttention(nn.Module):\n","    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n","        super(FullAttention, self).__init__()\n","        self.scale = scale\n","        self.mask_flag = mask_flag\n","        self.output_attention = output_attention\n","        self.dropout = nn.Dropout(attention_dropout)\n","        \n","    def forward(self, queries, keys, values, attn_mask):\n","        B, L, H, E = queries.shape\n","        _, S, _, D = values.shape\n","        scale = self.scale or 1./sqrt(E)\n","\n","        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n","        if self.mask_flag:\n","            if attn_mask is None:\n","                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n","\n","            scores.masked_fill_(attn_mask.mask, -np.inf)\n","\n","        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n","        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n","        \n","        if self.output_attention:\n","            return (V.contiguous(), A)\n","        else:\n","            return (V.contiguous(), None)\n","\n","class ProbAttention(nn.Module):\n","    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n","        super(ProbAttention, self).__init__()\n","        self.factor = factor\n","        self.scale = scale\n","        self.mask_flag = mask_flag\n","        self.output_attention = output_attention\n","        self.dropout = nn.Dropout(attention_dropout)\n","\n","    def _prob_QK(self, Q, K, sample_k, n_top): # n_top: c*ln(L_q)\n","        # Q [B, H, L, D]\n","        B, H, L_K, E = K.shape\n","        _, _, L_Q, _ = Q.shape\n","\n","        # calculate the sampled Q_K\n","        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n","        index_sample = torch.randint(L_K, (L_Q, sample_k)) # real U = U_part(factor*ln(L_k))*L_q\n","        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n","        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze(-2)\n","\n","        # find the Top_k query with sparisty measurement\n","        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n","        M_top = M.topk(n_top, sorted=False)[1]\n","\n","        # use the reduced Q to calculate Q_K\n","        Q_reduce = Q[torch.arange(B)[:, None, None],\n","                     torch.arange(H)[None, :, None],\n","                     M_top, :] # factor*ln(L_q)\n","        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1)) # factor*ln(L_q)*L_k\n","\n","        return Q_K, M_top\n","\n","    def _get_initial_context(self, V, L_Q):\n","        B, H, L_V, D = V.shape\n","        if not self.mask_flag:\n","            # V_sum = V.sum(dim=-2)\n","            V_sum = V.mean(dim=-2)\n","            contex = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n","        else: # use mask\n","            assert(L_Q == L_V) # requires that L_Q == L_V, i.e. for self-attention only\n","            contex = V.cumsum(dim=-2)\n","        return contex\n","\n","    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n","        B, H, L_V, D = V.shape\n","\n","        if self.mask_flag:\n","            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n","            scores.masked_fill_(attn_mask.mask, -np.inf)\n","\n","        attn = torch.softmax(scores, dim=-1) # nn.Softmax(dim=-1)(scores)\n","\n","        context_in[torch.arange(B)[:, None, None],\n","                   torch.arange(H)[None, :, None],\n","                   index, :] = torch.matmul(attn, V).type_as(context_in)\n","        if self.output_attention:\n","            attns = (torch.ones([B, H, L_V, L_V])/L_V).type_as(attn).to(attn.device)\n","            attns[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = attn\n","            return (context_in, attns)\n","        else:\n","            return (context_in, None)\n","\n","    def forward(self, queries, keys, values, attn_mask):\n","        B, L_Q, H, D = queries.shape\n","        _, L_K, _, _ = keys.shape\n","\n","        queries = queries.transpose(2,1)\n","        keys = keys.transpose(2,1)\n","        values = values.transpose(2,1)\n","\n","        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item() # c*ln(L_k)\n","        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item() # c*ln(L_q) \n","\n","        U_part = U_part if U_part<L_K else L_K\n","        u = u if u<L_Q else L_Q\n","        \n","        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u) \n","\n","        # add scale factor\n","        scale = self.scale or 1./sqrt(D)\n","        if scale is not None:\n","            scores_top = scores_top * scale\n","        # get the context\n","        context = self._get_initial_context(values, L_Q)\n","        # update the context with selected top_k queries\n","        context, attn = self._update_context(context, values, scores_top, index, L_Q, attn_mask)\n","        \n","        return context.transpose(2,1).contiguous(), attn\n","\n","\n","class AttentionLayer(nn.Module):\n","    def __init__(self, attention, d_model, n_heads, \n","                 d_keys=None, d_values=None, mix=False):\n","        super(AttentionLayer, self).__init__()\n","\n","        d_keys = d_keys or (d_model//n_heads)\n","        d_values = d_values or (d_model//n_heads)\n","\n","        self.inner_attention = attention\n","        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n","        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n","        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n","        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n","        self.n_heads = n_heads\n","        self.mix = mix\n","\n","    def forward(self, queries, keys, values, attn_mask):\n","        B, L, _ = queries.shape\n","        _, S, _ = keys.shape\n","        H = self.n_heads\n","\n","        queries = self.query_projection(queries).view(B, L, H, -1)\n","        keys = self.key_projection(keys).view(B, S, H, -1)\n","        values = self.value_projection(values).view(B, S, H, -1)\n","\n","        out, attn = self.inner_attention(\n","            queries,\n","            keys,\n","            values,\n","            attn_mask\n","        )\n","        if self.mix:\n","            out = out.transpose(2,1).contiguous()\n","        out = out.view(B, L, -1)\n","\n","        return self.out_projection(out), attn"],"metadata":{"id":"XU0tZNvepS6k","executionInfo":{"status":"ok","timestamp":1684950096601,"user_tz":-180,"elapsed":12,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### Encoder"],"metadata":{"id":"lUHJWjxQqMTa"}},{"cell_type":"code","source":["class ConvLayer(nn.Module):\n","    def __init__(self, c_in):\n","        super(ConvLayer, self).__init__()\n","        padding = 1 if torch.__version__>='1.5.0' else 2\n","        self.downConv = nn.Conv1d(in_channels=c_in,\n","                                  out_channels=c_in,\n","                                  kernel_size=3,\n","                                  padding=padding,\n","                                  padding_mode='circular')\n","        self.norm = nn.BatchNorm1d(c_in)\n","        self.activation = nn.ELU()\n","        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n","\n","    def forward(self, x):\n","        x = self.downConv(x.permute(0, 2, 1))\n","        x = self.norm(x)\n","        x = self.activation(x)\n","        x = self.maxPool(x)\n","        x = x.transpose(1,2)\n","        return x\n","\n","class EncoderLayer(nn.Module):\n","    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n","        super(EncoderLayer, self).__init__()\n","        d_ff = d_ff or 4*d_model\n","        self.attention = attention\n","        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n","        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = F.relu if activation == \"relu\" else F.gelu\n","\n","    def forward(self, x, attn_mask=None):\n","        # x [B, L, D]\n","        # x = x + self.dropout(self.attention(\n","        #     x, x, x,\n","        #     attn_mask = attn_mask\n","        # ))\n","        new_x, attn = self.attention(\n","            x, x, x,\n","            attn_mask = attn_mask\n","        )\n","        x = x + self.dropout(new_x)\n","\n","        y = x = self.norm1(x)\n","        y = self.dropout(self.activation(self.conv1(y.transpose(-1,1))))\n","        y = self.dropout(self.conv2(y).transpose(-1,1))\n","\n","        return self.norm2(x+y), attn\n","\n","class Encoder(nn.Module):\n","    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n","        super(Encoder, self).__init__()\n","        self.attn_layers = nn.ModuleList(attn_layers)\n","        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n","        self.norm = norm_layer\n","\n","    def forward(self, x, attn_mask=None):\n","        # x [B, L, D]\n","        attns = []\n","        if self.conv_layers is not None:\n","            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n","                x, attn = attn_layer(x, attn_mask=attn_mask)\n","                x = conv_layer(x)\n","                attns.append(attn)\n","            x, attn = self.attn_layers[-1](x, attn_mask=attn_mask)\n","            attns.append(attn)\n","        else:\n","            for attn_layer in self.attn_layers:\n","                x, attn = attn_layer(x, attn_mask=attn_mask)\n","                attns.append(attn)\n","\n","        if self.norm is not None:\n","            x = self.norm(x)\n","\n","        return x, attns"],"metadata":{"id":"vE0xlwNVqoXm","executionInfo":{"status":"ok","timestamp":1684950096601,"user_tz":-180,"elapsed":11,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Decoder"],"metadata":{"id":"P279t5OVqvQj"}},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n","                 dropout=0.1, activation=\"relu\"):\n","        super(DecoderLayer, self).__init__()\n","        d_ff = d_ff or 4*d_model\n","        self.self_attention = self_attention\n","        self.cross_attention = cross_attention\n","        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n","        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = F.relu if activation == \"relu\" else F.gelu\n","\n","    def forward(self, x, cross, x_mask=None, cross_mask=None):\n","        x = x + self.dropout(self.self_attention(\n","            x, x, x,\n","            attn_mask=x_mask\n","        )[0])\n","        x = self.norm1(x)\n","\n","        x = x + self.dropout(self.cross_attention(\n","            x, cross, cross,\n","            attn_mask=cross_mask\n","        )[0])\n","\n","        y = x = self.norm2(x)\n","        y = self.dropout(self.activation(self.conv1(y.transpose(-1,1))))\n","        y = self.dropout(self.conv2(y).transpose(-1,1))\n","\n","        return self.norm3(x+y)\n","\n","class Decoder(nn.Module):\n","    def __init__(self, layers, norm_layer=None):\n","        super(Decoder, self).__init__()\n","        self.layers = nn.ModuleList(layers)\n","        self.norm = norm_layer\n","\n","    def forward(self, x, cross, x_mask=None, cross_mask=None):\n","        for layer in self.layers:\n","            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n","\n","        if self.norm is not None:\n","            x = self.norm(x)\n","\n","        return x"],"metadata":{"id":"iV6yc8C5qwV5","executionInfo":{"status":"ok","timestamp":1684950097275,"user_tz":-180,"elapsed":684,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### Informer"],"metadata":{"id":"1OXVnN8Xq0cg"}},{"cell_type":"code","source":["class Informer(nn.Module):\n","    def __init__(self, enc_in, dec_in, c_out, out_len,\n","                factor=5, d_model=512, n_heads=8, e_layers=3, d_layers=2, d_ff=512, \n","                dropout=0.0, attn='prob', embed='fixed', freq='h', activation='gelu', \n","                output_attention = False, distil=True, mix=True,\n","                device=torch.device('cuda:0')):\n","        super(Informer, self).__init__()\n","        self.pred_len = out_len\n","        self.attn = attn\n","        self.output_attention = output_attention\n","\n","        # Encoding\n","        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, freq, dropout)\n","        self.dec_embedding = DataEmbedding(dec_in, d_model, embed, freq, dropout)\n","        # Attention\n","        Attn = ProbAttention if attn=='prob' else FullAttention\n","        # Encoder\n","        self.encoder = Encoder(\n","            [\n","                EncoderLayer(\n","                    AttentionLayer(Attn(False, factor, attention_dropout=dropout, output_attention=output_attention), \n","                                d_model, n_heads, mix=False),\n","                    d_model,\n","                    d_ff,\n","                    dropout=dropout,\n","                    activation=activation\n","                ) for l in range(e_layers)\n","            ],\n","            [\n","                ConvLayer(\n","                    d_model\n","                ) for l in range(e_layers-1)\n","            ] if distil else None,\n","            norm_layer=torch.nn.LayerNorm(d_model)\n","        )\n","        # Decoder\n","        self.decoder = Decoder(\n","            [\n","                DecoderLayer(\n","                    AttentionLayer(Attn(True, factor, attention_dropout=dropout, output_attention=False), \n","                                d_model, n_heads, mix=mix),\n","                    AttentionLayer(FullAttention(False, factor, attention_dropout=dropout, output_attention=False), \n","                                d_model, n_heads, mix=False),\n","                    d_model,\n","                    d_ff,\n","                    dropout=dropout,\n","                    activation=activation,\n","                )\n","                for l in range(d_layers)\n","            ],\n","            norm_layer=torch.nn.LayerNorm(d_model)\n","        )\n","        # self.end_conv1 = nn.Conv1d(in_channels=label_len+out_len, out_channels=out_len, kernel_size=1, bias=True)\n","        # self.end_conv2 = nn.Conv1d(in_channels=d_model, out_channels=c_out, kernel_size=1, bias=True)\n","        self.projection = nn.Linear(d_model, c_out, bias=True)\n","        \n","    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, \n","                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n","        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n","        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n","\n","        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n","        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n","        dec_out = self.projection(dec_out)\n","        \n","        # dec_out = self.end_conv1(dec_out)\n","        # dec_out = self.end_conv2(dec_out.transpose(2,1)).transpose(1,2)\n","        if self.output_attention:\n","            return dec_out[:,-self.pred_len:,:], attns\n","        else:\n","            return dec_out[:,-self.pred_len:,:] # [B, L, D]"],"metadata":{"id":"9eXJMbtFq1zh","executionInfo":{"status":"ok","timestamp":1684950097275,"user_tz":-180,"elapsed":4,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## Training and Evaluation"],"metadata":{"id":"xF7JUjfxbfyE"}},{"cell_type":"code","source":["def mape(y_preds, y_true):\n","  epsilon = 1.17e-06\n","  abs_diff = torch.abs(y_preds - y_true)\n","  abs_per_error = abs_diff / torch.clamp(torch.abs(y_true), min=epsilon)\n","  mape = torch.sum(abs_per_error) / y_true.numel()\n","\n","  return mape \n","\n","def train_step(model,\n","               dataloader, \n","               optimizer, \n","               device):\n","  \n","  model.train()\n","  loss = 0\n","  for batch, (X_enc, X_time_enc, X_dec, X_time_dec, Y) in enumerate(dataloader):\n","    X_enc, X_time_enc, X_dec, X_time_dec, Y = X_enc.to(device), X_time_enc.to(device), X_dec.to(device), X_time_dec.to(device), Y.to(device)\n","    Y_preds = model(x_enc=X_enc, x_mark_enc=X_time_enc, x_dec=X_dec, x_mark_dec=X_time_dec).squeeze()\n","    batch_loss = mape(Y_preds, Y) \n","    loss += batch_loss.item()\n","    optimizer.zero_grad()\n","    batch_loss.backward()\n","    optimizer.step()\n","  \n","  loss /= len(dataloader)\n","  return loss\n","\n","def val_step(model, dataloader, device):\n","  model.eval()\n","  val_loss = 0\n","  with torch.inference_mode():\n","    for batch, (X_enc, X_time_enc, X_dec, X_time_dec, Y) in enumerate(dataloader):\n","      X_enc, X_time_enc, X_dec, X_time_dec, Y = X_enc.to(device), X_time_enc.to(device), X_dec.to(device), X_time_dec.to(device), Y.to(device)\n","      Y_preds = model(x_enc=X_enc, x_mark_enc=X_time_enc, x_dec=X_dec, x_mark_dec=X_time_dec).squeeze()\n","\n","      y_preds_unscaled = Y_preds * scaler.max_abs_[TARGET_POS]\n","      y_true_unscaled  = Y * scaler.max_abs_[TARGET_POS] \n","\n","      batch_loss = 100 * mape(y_preds_unscaled, y_true_unscaled)     # loss_fn(Y_preds, Y) \n","      val_loss += batch_loss.item()\n","  \n","  val_loss /= len(dataloader)\n","  return val_loss\n","\n","def train(model, \n","          train_dataloader,\n","          val_dataloader,\n","          optimizer,\n","          scheduler,\n","          epochs,\n","          patience,\n","          device,\n","          path):\n","  \n","  results = {\n","      \"loss\": [],\n","      \"val_loss\": []\n","  }\n","\n","  for epoch in tqdm(range(epochs)):\n","    flag = 0\n","    loss = train_step(model=model,\n","                      dataloader=train_dataloader,\n","                      optimizer=optimizer,\n","                      device=device)\n","\n","    val_loss = val_step(model=model,\n","                        dataloader=val_dataloader,\n","                        device=device)\n","    scheduler.step(val_loss)\n","    \n","    results['loss'].append(loss)\n","    results['val_loss'].append(val_loss)\n","    if epoch == 0:\n","      best_val_loss = val_loss\n","      best_epoch = -1\n","      checkpoint(model, optimizer, path)\n","      flag = 1\n","      print(f\"Epoch: {epoch+1}/{epochs} | Loss: {loss:.4f} | Val loss: {val_loss:.4f} - *Checkpoint*\")\n","    else:\n","      if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        best_epoch = epoch\n","        checkpoint(model, optimizer, path)\n","        flag = 1\n","        print(f\"Epoch: {epoch+1}/{epochs} | Loss: {loss:.4f} | Val loss: {val_loss:.4f} - *Checkpoint*\")\n","      elif epoch - best_epoch > patience:\n","        print(f\"\\nEarly stopping applied at epoch {epoch}.\")\n","        break\n","    if flag == 0:\n","      print(f\"Epoch: {epoch+1}/{epochs} | Loss: {loss:.4f} | Val loss: {val_loss:.4f}\")\n","  \n","  return results\n","\n","def checkpoint(model, optimizer, filepath):\n","  torch.save({\n","    \"optimizer\": optimizer.state_dict(),\n","    \"model\": model.state_dict()\n","  }, filepath)"],"metadata":{"id":"s2egVAsMZW6Z","executionInfo":{"status":"ok","timestamp":1684950097276,"user_tz":-180,"elapsed":4,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# MAIN"],"metadata":{"id":"kf-rtjthrvMY"}},{"cell_type":"code","source":["# Encoder \n","DAYS_BACK_ENC = 2\n","SKIP_DAYS_BACK_ENC = 2\n","SKIP_STEPS_BACK_ENC = 24 * SKIP_DAYS_BACK_ENC\n","STEPS_BACK_ENC = 24 * DAYS_BACK_ENC\n","LAST_STEP_BACK_ENC = STEPS_BACK_ENC + SKIP_STEPS_BACK_ENC\n","\n","# Decoder\n","DAYS_BACK_DEC = 2\n","LAST_STEP_BACK_DEC = 24 * DAYS_BACK_DEC\n","\n","# Forward\n","DAYS_TO_SKIP = 0\n","STEPS_FORWARD = 24    # 1 day\n","SKIP_STEPS_FORWARD = 24 * DAYS_TO_SKIP\n","LAST_STEP_FORWARD = STEPS_FORWARD + SKIP_STEPS_FORWARD\n","\n","LAST_STEP_BACK = LAST_STEP_BACK_ENC + LAST_STEP_BACK_DEC + STEPS_FORWARD\n","\n","# keep 1 year for testing\n","START_TEST_DATE = pd.to_datetime('2018-01-01') - pd.to_timedelta(SKIP_STEPS_FORWARD+1, 'h')\n","END_TEST_DATE = START_TEST_DATE + pd.DateOffset(years=1)\n","\n","END_VAL_DATE = START_TEST_DATE - pd.to_timedelta(1, 'h')\n","START_VAL_DATE = pd.to_datetime('2017-01-01') - pd.to_timedelta(LAST_STEP_FORWARD, 'h')\n","\n","START_TRAIN_DATE = pd.to_datetime('2010-10-01')\n","END_TRAIN_DATE = START_VAL_DATE - pd.to_timedelta(1, 'h')\n","\n","# END_TRAIN_DATE = START_TEST_DATE - pd.to_timedelta(1, 'h')\n","# START_TRAIN_DATE = pd.to_datetime('2017-01-01') - pd.to_timedelta(SKIP_STEPS_FORWARD+1, 'h')\n","\n","# START_VAL_DATE = pd.to_datetime('2010-10-01')\n","# END_VAL_DATE = START_TRAIN_DATE - pd.to_timedelta(1, 'h')\n","\n","TARGET = \"TOTAL_CONS\"\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","print(f\"Train from {START_TRAIN_DATE} to {END_TRAIN_DATE}\")\n","print(f\"Validation from {START_VAL_DATE} to {END_VAL_DATE}\")\n","print(f\"Test from {START_TEST_DATE} to {END_TEST_DATE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cy3AXiz2yQjN","executionInfo":{"status":"ok","timestamp":1684951187968,"user_tz":-180,"elapsed":457,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}},"outputId":"333cc2ec-e509-46ba-e351-03d0007a925a"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Train from 2010-10-01 00:00:00 to 2016-12-30 23:00:00\n","Validation from 2016-12-31 00:00:00 to 2017-12-31 22:00:00\n","Test from 2017-12-31 23:00:00 to 2018-12-31 23:00:00\n"]}]},{"cell_type":"code","source":["load_df = pd.read_csv(\"/content/FINAL_DATASET_2.csv\")\n","load_df.set_index(pd.to_datetime(load_df[\"Timestamp\"]), inplace=True)\n","load_df.drop(\"Timestamp\", axis=1, inplace=True)\n","\n","TIME_COLS = list(load_df.drop([TARGET, 'temp', 'humidity'], axis=1).columns)\n","TARGET_POS = np.where(load_df.columns == TARGET)[0][0]\n","TIME_POS = list(np.where(load_df.columns == col)[0][0] for col in TIME_COLS)\n","NO_TARGET_POS = list(np.where(load_df.columns == col)[0][0] for col in load_df.drop(TARGET, axis=1).columns)\n","\n","\n","data_df_scaled, scaler = scale_data(load_df,\n","                                    # START_VAL_DATE,\n","                                    # END_TRAIN_DATE,\n","                                    # START_TEST_DATE,\n","                                    # END_TEST_DATE)\n","                                    START_TRAIN_DATE,\n","                                    END_VAL_DATE,\n","                                    START_TEST_DATE,\n","                                    END_TEST_DATE)"],"metadata":{"id":"z2Hv5lXg7EtD","executionInfo":{"status":"ok","timestamp":1684951189744,"user_tz":-180,"elapsed":2,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["data_df_scaled_reframed_enc = reframe_enc_data(data_df_scaled.copy(), \n","                                               TARGET,\n","                                               SKIP_STEPS_BACK_ENC,\n","                                               LAST_STEP_BACK_ENC,\n","                                               LAST_STEP_FORWARD)\n","\n","data_df_scaled_reframed_dec = reframe_dec_data(data_df_scaled.copy(),\n","                                               LAST_STEP_BACK_DEC,\n","                                               LAST_STEP_FORWARD)\n","\n","common_index = data_df_scaled_reframed_enc.index.intersection(data_df_scaled_reframed_dec.index)\n","data_df_scaled_reframed_enc = data_df_scaled_reframed_enc.loc[common_index]\n","data_df_scaled_reframed_dec = data_df_scaled_reframed_dec.loc[common_index]\n","\n","print(data_df_scaled_reframed_enc.shape, data_df_scaled_reframed_dec.shape)"],"metadata":{"id":"6cRrYOkQmUgs","executionInfo":{"status":"ok","timestamp":1684951197254,"user_tz":-180,"elapsed":1936,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fa02900a-d4fe-4417-dfec-1820b44cc393"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["(72217, 384) (72217, 576)\n"]}]},{"cell_type":"code","source":["data_df_scaled_reframed_dec.iloc[:5, 2*24:3*24 + 1]"],"metadata":{"id":"jPK0RSRs7xwt","executionInfo":{"status":"aborted","timestamp":1684950100314,"user_tz":-180,"elapsed":5,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# X_train_3D_dec[0, :5, TARGET_POS]"],"metadata":{"id":"rBL5jUUNAsmS","executionInfo":{"status":"aborted","timestamp":1684950100315,"user_tz":-180,"elapsed":6,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_3D_enc, X_val_3D_enc, X_test_3D_enc = split_enc_data(data_df_scaled_reframed_enc,\n","                                                              # START_VAL_DATE,\n","                                                              # END_VAL_DATE,\n","                                                              # START_TRAIN_DATE,\n","                                                              # END_TRAIN_DATE,\n","                                                              # START_TEST_DATE,\n","                                                              # END_TEST_DATE,\n","                                                              # STEPS_BACK_ENC)\n","                                                              START_TRAIN_DATE,\n","                                                              END_TRAIN_DATE,\n","                                                              START_VAL_DATE,\n","                                                              END_VAL_DATE,\n","                                                              START_TEST_DATE,\n","                                                              END_TEST_DATE,\n","                                                              STEPS_BACK_ENC)\n","\n","X_train_3D_dec_orig, X_val_3D_dec_orig, X_test_3D_dec_orig = split_dec_data(data_df_scaled_reframed_dec,\n","                                                                            # START_VAL_DATE,\n","                                                                            # END_VAL_DATE,\n","                                                                            # START_TRAIN_DATE,\n","                                                                            # END_TRAIN_DATE,\n","                                                                            # START_TEST_DATE,\n","                                                                            # END_TEST_DATE,\n","                                                                            # STEPS_FORWARD,\n","                                                                            # LAST_STEP_BACK_DEC)\n","                                                                            START_TRAIN_DATE,\n","                                                                            END_TRAIN_DATE,\n","                                                                            START_VAL_DATE,\n","                                                                            END_VAL_DATE,\n","                                                                            START_TEST_DATE,\n","                                                                            END_TEST_DATE,\n","                                                                            STEPS_FORWARD,\n","                                                                            LAST_STEP_BACK_DEC)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dmg9Muc20ozz","executionInfo":{"status":"ok","timestamp":1684951223804,"user_tz":-180,"elapsed":1126,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}},"outputId":"102cefef-bf7e-4bfd-e4cf-d6b6331828be"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["(54697, 48, 8)\n","(8783, 48, 8)\n","(8737, 48, 8)\n","(54697, 72, 8)\n","(8783, 72, 8)\n","(8737, 72, 8)\n"]}]},{"cell_type":"code","source":["y_train = X_train_3D_dec_orig[:, :LAST_STEP_FORWARD, TARGET_POS]\n","X_train_3D_dec = np.concatenate(\n","    (\n","        X_train_3D_dec_orig[:, LAST_STEP_FORWARD:, TARGET_POS].reshape(-1, LAST_STEP_BACK_DEC, 1),\n","        X_train_3D_dec_orig[:, :LAST_STEP_BACK_DEC, NO_TARGET_POS]\n","    ), axis=2\n",")\n","\n","y_val = X_val_3D_dec_orig[:, :LAST_STEP_FORWARD, TARGET_POS]\n","X_val_3D_dec = np.concatenate(\n","    (\n","        X_val_3D_dec_orig[:, LAST_STEP_FORWARD:, TARGET_POS].reshape(-1, LAST_STEP_BACK_DEC, 1),\n","        X_val_3D_dec_orig[:, :LAST_STEP_BACK_DEC, NO_TARGET_POS]\n","    ), axis=2\n",")\n","\n","y_test = X_test_3D_dec_orig[:, :LAST_STEP_FORWARD, TARGET_POS]\n","X_test_3D_dec = np.concatenate(\n","    (\n","        X_test_3D_dec_orig[:, LAST_STEP_FORWARD:, TARGET_POS].reshape(-1, LAST_STEP_BACK_DEC, 1),\n","        X_test_3D_dec_orig[:, :LAST_STEP_BACK_DEC, NO_TARGET_POS]\n","    ), axis=2\n",")\n","\n","print(X_train_3D_dec.shape, X_val_3D_dec.shape, X_test_3D_dec.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GpDOc1O09DhS","executionInfo":{"status":"ok","timestamp":1684951247710,"user_tz":-180,"elapsed":457,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}},"outputId":"2321affc-cfa1-4c43-b5f9-cc91e05cf309"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["(54697, 48, 8) (8783, 48, 8) (8737, 48, 8)\n"]}]},{"cell_type":"code","source":["X_train_3D_enc = np.flip(X_train_3D_enc, axis=1)  #.reshape(-1, 8, LAST_STEP_BACK_DEC)\n","X_val_3D_enc = np.flip(X_val_3D_enc, axis=1)  #.reshape(-1, 8, LAST_STEP_BACK_DEC)\n","X_test_3D_enc = np.flip(X_test_3D_enc, axis=1)  #.reshape(-1, 8, LAST_STEP_BACK_DEC)\n","X_train_3D_dec = np.flip(X_train_3D_dec, axis=1)  #.reshape(-1, 8, LAST_STEP_BACK_DEC)\n","X_val_3D_dec = np.flip(X_val_3D_dec, axis=1)  #.reshape(-1, 8, LAST_STEP_BACK_DEC)\n","X_test_3D_dec = np.flip(X_test_3D_dec, axis=1)  #.reshape(-1, 8, LAST_STEP_BACK_DEC)\n","\n","y_train = np.flip(y_train, axis=1)\n","y_val = np.flip(y_val, axis=1)\n","y_test = np.flip(y_test, axis=1)"],"metadata":{"id":"BhqJAKwoBeeD","executionInfo":{"status":"ok","timestamp":1684951251404,"user_tz":-180,"elapsed":751,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 1024\n","\n","# *** DATALOADERS ***\n","train_dataset = LoadDataset(X_3D_enc=X_train_3D_enc.copy(), \n","                            X_3D_time_enc=X_train_3D_enc[:, :, TIME_POS].copy(),\n","                            X_3D_dec=X_train_3D_dec.copy(), \n","                            X_3D_time_dec=X_train_3D_dec[:, :, TIME_POS].copy(),\n","                            y=y_train.copy())\n","train_dataloader = DataLoader(dataset=train_dataset, \n","                              batch_size=BATCH_SIZE,\n","                              shuffle=True)\n","\n","val_dataset = LoadDataset(X_3D_enc=X_val_3D_enc.copy(), \n","                          X_3D_time_enc=X_val_3D_enc[:, :, TIME_POS].copy(),\n","                          X_3D_dec=X_val_3D_dec.copy(), \n","                          X_3D_time_dec=X_val_3D_dec[:, :, TIME_POS].copy(),\n","                          y=y_val.copy())\n","val_dataloader = DataLoader(dataset=val_dataset, \n","                            batch_size=BATCH_SIZE,\n","                            shuffle=False)"],"metadata":{"id":"METvM50Zy9fk","executionInfo":{"status":"ok","timestamp":1684951257903,"user_tz":-180,"elapsed":1351,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["# TRAIN"],"metadata":{"id":"uPvm6Voxwk-r"}},{"cell_type":"code","source":["EPOCHS = 1000\n","PATIENCE = 31\n","PATH = \"model.pth\"\n","\n","model = Informer(enc_in=8, dec_in=8, c_out=24, out_len=1,\n","                 d_model=64, n_heads=8, e_layers=1, d_layers=1, d_ff=64, attn='prob').to(device)\n","\n","optimizer = torch.optim.Adam(params=model.parameters(),\n","                             lr=1e-3,\n","                             weight_decay=0)\n","# optimizer = t.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.33, patience=20, verbose=True)\n","# loss_fn = MeanAbsolutePercentageError().to(device)   # MeanAbsolutePercentageError(), L1Loss(), MSELoss()\n","\n","!rm -rf \"model.pth\"\n","\n","model_results = train(model=model, \n","                      train_dataloader=train_dataloader,\n","                      val_dataloader=val_dataloader,\n","                      optimizer=optimizer,\n","                      scheduler=scheduler,\n","                      epochs=EPOCHS,\n","                      patience=PATIENCE,\n","                      device=device,\n","                      path=PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["be232ecaf7e94be986d8b58b38bbdf8a","f385ac54f20646d085610599c4c04290","56cf893ef72d4233979d45b3942b7bae","e444191b66c44ddc9984967d15027d03","7f67dc351c4a434fa71737593b4ea9e5","bb6511746e7540f3a3baf99f1e89241a","c97d144f6a35435fadc5734b1505ddbf","089cf9477a654829a5ba3717c8bad709","7ecbd0457f2943cca74e9dfe3bd8ad0c","1273668c5c8c4554b7b520d9845600a4","4644b636fc25463cafc1cb2a386ad874"]},"id":"vPGP0lMEvtgT","executionInfo":{"status":"ok","timestamp":1684952989839,"user_tz":-180,"elapsed":1717648,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}},"outputId":"753d7da7-197d-4afb-e1d9-e5d689df4132"},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1000 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be232ecaf7e94be986d8b58b38bbdf8a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 1/1000 | Loss: 0.2926 | Val loss: 18.8822 - *Checkpoint*\n","Epoch: 2/1000 | Loss: 0.1586 | Val loss: 16.6571 - *Checkpoint*\n","Epoch: 3/1000 | Loss: 0.1515 | Val loss: 16.4987 - *Checkpoint*\n","Epoch: 4/1000 | Loss: 0.1360 | Val loss: 13.8486 - *Checkpoint*\n","Epoch: 5/1000 | Loss: 0.1057 | Val loss: 12.0381 - *Checkpoint*\n","Epoch: 6/1000 | Loss: 0.1019 | Val loss: 10.7985 - *Checkpoint*\n","Epoch: 7/1000 | Loss: 0.0776 | Val loss: 9.2250 - *Checkpoint*\n","Epoch: 8/1000 | Loss: 0.0719 | Val loss: 7.4359 - *Checkpoint*\n","Epoch: 9/1000 | Loss: 0.0696 | Val loss: 7.7626\n","Epoch: 10/1000 | Loss: 0.0724 | Val loss: 7.0476 - *Checkpoint*\n","Epoch: 11/1000 | Loss: 0.0624 | Val loss: 7.0871\n","Epoch: 12/1000 | Loss: 0.0598 | Val loss: 6.9922 - *Checkpoint*\n","Epoch: 13/1000 | Loss: 0.0638 | Val loss: 6.6646 - *Checkpoint*\n","Epoch: 14/1000 | Loss: 0.0650 | Val loss: 6.8257\n","Epoch: 15/1000 | Loss: 0.0590 | Val loss: 6.8649\n","Epoch: 16/1000 | Loss: 0.0526 | Val loss: 5.9650 - *Checkpoint*\n","Epoch: 17/1000 | Loss: 0.0495 | Val loss: 5.5201 - *Checkpoint*\n","Epoch: 18/1000 | Loss: 0.0482 | Val loss: 5.5332\n","Epoch: 19/1000 | Loss: 0.0464 | Val loss: 5.6107\n","Epoch: 20/1000 | Loss: 0.0456 | Val loss: 5.5177 - *Checkpoint*\n","Epoch: 21/1000 | Loss: 0.0445 | Val loss: 5.0622 - *Checkpoint*\n","Epoch: 22/1000 | Loss: 0.0431 | Val loss: 4.8721 - *Checkpoint*\n","Epoch: 23/1000 | Loss: 0.0423 | Val loss: 4.9885\n","Epoch: 24/1000 | Loss: 0.0423 | Val loss: 4.9206\n","Epoch: 25/1000 | Loss: 0.0422 | Val loss: 4.6962 - *Checkpoint*\n","Epoch: 26/1000 | Loss: 0.0413 | Val loss: 4.6860 - *Checkpoint*\n","Epoch: 27/1000 | Loss: 0.0401 | Val loss: 4.7830\n","Epoch: 28/1000 | Loss: 0.0399 | Val loss: 4.5942 - *Checkpoint*\n","Epoch: 29/1000 | Loss: 0.0392 | Val loss: 5.1343\n","Epoch: 30/1000 | Loss: 0.0393 | Val loss: 4.6131\n","Epoch: 31/1000 | Loss: 0.0394 | Val loss: 4.4271 - *Checkpoint*\n","Epoch: 32/1000 | Loss: 0.0381 | Val loss: 4.4613\n","Epoch: 33/1000 | Loss: 0.0377 | Val loss: 4.3822 - *Checkpoint*\n","Epoch: 34/1000 | Loss: 0.0371 | Val loss: 4.2761 - *Checkpoint*\n","Epoch: 35/1000 | Loss: 0.0377 | Val loss: 4.2406 - *Checkpoint*\n","Epoch: 36/1000 | Loss: 0.0376 | Val loss: 4.7079\n","Epoch: 37/1000 | Loss: 0.0375 | Val loss: 4.3840\n","Epoch: 38/1000 | Loss: 0.0369 | Val loss: 4.2622\n","Epoch: 39/1000 | Loss: 0.0363 | Val loss: 4.2395 - *Checkpoint*\n","Epoch: 40/1000 | Loss: 0.0363 | Val loss: 4.9289\n","Epoch: 41/1000 | Loss: 0.0367 | Val loss: 4.4539\n","Epoch: 42/1000 | Loss: 0.0361 | Val loss: 4.4354\n","Epoch: 43/1000 | Loss: 0.0356 | Val loss: 4.1541 - *Checkpoint*\n","Epoch: 44/1000 | Loss: 0.0361 | Val loss: 4.0739 - *Checkpoint*\n","Epoch: 45/1000 | Loss: 0.0354 | Val loss: 4.1334\n","Epoch: 46/1000 | Loss: 0.0360 | Val loss: 4.0702 - *Checkpoint*\n","Epoch: 47/1000 | Loss: 0.0352 | Val loss: 4.4808\n","Epoch: 48/1000 | Loss: 0.0353 | Val loss: 4.5029\n","Epoch: 49/1000 | Loss: 0.0350 | Val loss: 4.1861\n","Epoch: 50/1000 | Loss: 0.0345 | Val loss: 4.1781\n","Epoch: 51/1000 | Loss: 0.0352 | Val loss: 4.0868\n","Epoch: 52/1000 | Loss: 0.0347 | Val loss: 4.1609\n","Epoch: 53/1000 | Loss: 0.0343 | Val loss: 4.0518 - *Checkpoint*\n","Epoch: 54/1000 | Loss: 0.0352 | Val loss: 4.3687\n","Epoch: 55/1000 | Loss: 0.0347 | Val loss: 4.1553\n","Epoch: 56/1000 | Loss: 0.0343 | Val loss: 4.0192 - *Checkpoint*\n","Epoch: 57/1000 | Loss: 0.0345 | Val loss: 4.1208\n","Epoch: 58/1000 | Loss: 0.0345 | Val loss: 4.4976\n","Epoch: 59/1000 | Loss: 0.0340 | Val loss: 4.1531\n","Epoch: 60/1000 | Loss: 0.0335 | Val loss: 4.1463\n","Epoch: 61/1000 | Loss: 0.0340 | Val loss: 3.9210 - *Checkpoint*\n","Epoch: 62/1000 | Loss: 0.0340 | Val loss: 4.0984\n","Epoch: 63/1000 | Loss: 0.0338 | Val loss: 4.0270\n","Epoch: 64/1000 | Loss: 0.0339 | Val loss: 3.9427\n","Epoch: 65/1000 | Loss: 0.0331 | Val loss: 3.8798 - *Checkpoint*\n","Epoch: 66/1000 | Loss: 0.0328 | Val loss: 3.8361 - *Checkpoint*\n","Epoch: 67/1000 | Loss: 0.0334 | Val loss: 3.8574\n","Epoch: 68/1000 | Loss: 0.0338 | Val loss: 3.9296\n","Epoch: 69/1000 | Loss: 0.0326 | Val loss: 3.8606\n","Epoch: 70/1000 | Loss: 0.0329 | Val loss: 3.8142 - *Checkpoint*\n","Epoch: 71/1000 | Loss: 0.0329 | Val loss: 4.2420\n","Epoch: 72/1000 | Loss: 0.0329 | Val loss: 3.8343\n","Epoch: 73/1000 | Loss: 0.0326 | Val loss: 4.0245\n","Epoch: 74/1000 | Loss: 0.0326 | Val loss: 3.8893\n","Epoch: 75/1000 | Loss: 0.0322 | Val loss: 3.7577 - *Checkpoint*\n","Epoch: 76/1000 | Loss: 0.0332 | Val loss: 3.8047\n","Epoch: 77/1000 | Loss: 0.0323 | Val loss: 3.9098\n","Epoch: 78/1000 | Loss: 0.0322 | Val loss: 4.0868\n","Epoch: 79/1000 | Loss: 0.0324 | Val loss: 3.7449 - *Checkpoint*\n","Epoch: 80/1000 | Loss: 0.0320 | Val loss: 3.8033\n","Epoch: 81/1000 | Loss: 0.0323 | Val loss: 4.4348\n","Epoch: 82/1000 | Loss: 0.0320 | Val loss: 3.9141\n","Epoch: 83/1000 | Loss: 0.0318 | Val loss: 3.8515\n","Epoch: 84/1000 | Loss: 0.0315 | Val loss: 3.7859\n","Epoch: 85/1000 | Loss: 0.0325 | Val loss: 3.8150\n","Epoch: 86/1000 | Loss: 0.0317 | Val loss: 3.7667\n","Epoch: 87/1000 | Loss: 0.0315 | Val loss: 3.7500\n","Epoch: 88/1000 | Loss: 0.0316 | Val loss: 3.8221\n","Epoch: 89/1000 | Loss: 0.0323 | Val loss: 3.7692\n","Epoch: 90/1000 | Loss: 0.0315 | Val loss: 3.7791\n","Epoch: 91/1000 | Loss: 0.0309 | Val loss: 3.9125\n","Epoch: 92/1000 | Loss: 0.0325 | Val loss: 3.8872\n","Epoch: 93/1000 | Loss: 0.0313 | Val loss: 3.7495\n","Epoch: 94/1000 | Loss: 0.0309 | Val loss: 3.6882 - *Checkpoint*\n","Epoch: 95/1000 | Loss: 0.0313 | Val loss: 3.7475\n","Epoch: 96/1000 | Loss: 0.0308 | Val loss: 3.7088\n","Epoch: 97/1000 | Loss: 0.0321 | Val loss: 3.7388\n","Epoch: 98/1000 | Loss: 0.0312 | Val loss: 4.0900\n","Epoch: 99/1000 | Loss: 0.0311 | Val loss: 3.6660 - *Checkpoint*\n","Epoch: 100/1000 | Loss: 0.0305 | Val loss: 4.0647\n","Epoch: 101/1000 | Loss: 0.0313 | Val loss: 3.6740\n","Epoch: 102/1000 | Loss: 0.0312 | Val loss: 4.1063\n","Epoch: 103/1000 | Loss: 0.0306 | Val loss: 3.9650\n","Epoch: 104/1000 | Loss: 0.0305 | Val loss: 3.6213 - *Checkpoint*\n","Epoch: 105/1000 | Loss: 0.0305 | Val loss: 3.7647\n","Epoch: 106/1000 | Loss: 0.0307 | Val loss: 3.6959\n","Epoch: 107/1000 | Loss: 0.0310 | Val loss: 3.7276\n","Epoch: 108/1000 | Loss: 0.0303 | Val loss: 3.7690\n","Epoch: 109/1000 | Loss: 0.0303 | Val loss: 3.7418\n","Epoch: 110/1000 | Loss: 0.0305 | Val loss: 3.6930\n","Epoch: 111/1000 | Loss: 0.0304 | Val loss: 3.7360\n","Epoch: 112/1000 | Loss: 0.0308 | Val loss: 3.6770\n","Epoch: 113/1000 | Loss: 0.0305 | Val loss: 3.9286\n","Epoch: 114/1000 | Loss: 0.0305 | Val loss: 3.6760\n","Epoch: 115/1000 | Loss: 0.0302 | Val loss: 3.6416\n","Epoch: 116/1000 | Loss: 0.0302 | Val loss: 4.1913\n","Epoch: 117/1000 | Loss: 0.0301 | Val loss: 3.6076 - *Checkpoint*\n","Epoch: 118/1000 | Loss: 0.0303 | Val loss: 3.6359\n","Epoch: 119/1000 | Loss: 0.0299 | Val loss: 3.7146\n","Epoch: 120/1000 | Loss: 0.0301 | Val loss: 3.6626\n","Epoch: 121/1000 | Loss: 0.0300 | Val loss: 3.5993 - *Checkpoint*\n","Epoch: 122/1000 | Loss: 0.0297 | Val loss: 3.6115\n","Epoch: 123/1000 | Loss: 0.0298 | Val loss: 3.6537\n","Epoch: 124/1000 | Loss: 0.0301 | Val loss: 3.7703\n","Epoch: 125/1000 | Loss: 0.0301 | Val loss: 3.6969\n","Epoch: 126/1000 | Loss: 0.0299 | Val loss: 3.7728\n","Epoch: 127/1000 | Loss: 0.0300 | Val loss: 3.9567\n","Epoch: 128/1000 | Loss: 0.0299 | Val loss: 3.7070\n","Epoch: 129/1000 | Loss: 0.0299 | Val loss: 4.0021\n","Epoch: 130/1000 | Loss: 0.0301 | Val loss: 3.6681\n","Epoch: 131/1000 | Loss: 0.0294 | Val loss: 3.5896 - *Checkpoint*\n","Epoch: 132/1000 | Loss: 0.0296 | Val loss: 3.8871\n","Epoch: 133/1000 | Loss: 0.0294 | Val loss: 3.5702 - *Checkpoint*\n","Epoch: 134/1000 | Loss: 0.0295 | Val loss: 3.5823\n","Epoch: 135/1000 | Loss: 0.0297 | Val loss: 3.6631\n","Epoch: 136/1000 | Loss: 0.0294 | Val loss: 3.6632\n","Epoch: 137/1000 | Loss: 0.0296 | Val loss: 3.5072 - *Checkpoint*\n","Epoch: 138/1000 | Loss: 0.0292 | Val loss: 3.5917\n","Epoch: 139/1000 | Loss: 0.0292 | Val loss: 3.5915\n","Epoch: 140/1000 | Loss: 0.0295 | Val loss: 3.7393\n","Epoch: 141/1000 | Loss: 0.0291 | Val loss: 3.6188\n","Epoch: 142/1000 | Loss: 0.0296 | Val loss: 3.9807\n","Epoch: 143/1000 | Loss: 0.0293 | Val loss: 3.5768\n","Epoch: 144/1000 | Loss: 0.0289 | Val loss: 3.5110\n","Epoch: 145/1000 | Loss: 0.0285 | Val loss: 3.5399\n","Epoch: 146/1000 | Loss: 0.0291 | Val loss: 4.3292\n","Epoch: 147/1000 | Loss: 0.0293 | Val loss: 3.5467\n","Epoch: 148/1000 | Loss: 0.0287 | Val loss: 3.5478\n","Epoch: 149/1000 | Loss: 0.0285 | Val loss: 3.7318\n","Epoch: 150/1000 | Loss: 0.0290 | Val loss: 3.5735\n","Epoch: 151/1000 | Loss: 0.0280 | Val loss: 3.5526\n","Epoch: 152/1000 | Loss: 0.0284 | Val loss: 3.4344 - *Checkpoint*\n","Epoch: 153/1000 | Loss: 0.0284 | Val loss: 3.6525\n","Epoch: 154/1000 | Loss: 0.0287 | Val loss: 3.4795\n","Epoch: 155/1000 | Loss: 0.0291 | Val loss: 3.7340\n","Epoch: 156/1000 | Loss: 0.0279 | Val loss: 3.5366\n","Epoch: 157/1000 | Loss: 0.0280 | Val loss: 3.6604\n","Epoch: 158/1000 | Loss: 0.0281 | Val loss: 3.4148 - *Checkpoint*\n","Epoch: 159/1000 | Loss: 0.0280 | Val loss: 3.4126 - *Checkpoint*\n","Epoch: 160/1000 | Loss: 0.0277 | Val loss: 3.4560\n","Epoch: 161/1000 | Loss: 0.0274 | Val loss: 3.6799\n","Epoch: 162/1000 | Loss: 0.0278 | Val loss: 3.4279\n","Epoch: 163/1000 | Loss: 0.0280 | Val loss: 3.3641 - *Checkpoint*\n","Epoch: 164/1000 | Loss: 0.0276 | Val loss: 3.8274\n","Epoch: 165/1000 | Loss: 0.0279 | Val loss: 3.4157\n","Epoch: 166/1000 | Loss: 0.0271 | Val loss: 3.6061\n","Epoch: 167/1000 | Loss: 0.0272 | Val loss: 3.7241\n","Epoch: 168/1000 | Loss: 0.0269 | Val loss: 3.4203\n","Epoch: 169/1000 | Loss: 0.0273 | Val loss: 3.7672\n","Epoch: 170/1000 | Loss: 0.0272 | Val loss: 3.5649\n","Epoch: 171/1000 | Loss: 0.0268 | Val loss: 3.3511 - *Checkpoint*\n","Epoch: 172/1000 | Loss: 0.0269 | Val loss: 3.8241\n","Epoch: 173/1000 | Loss: 0.0267 | Val loss: 3.3907\n","Epoch: 174/1000 | Loss: 0.0268 | Val loss: 3.4124\n","Epoch: 175/1000 | Loss: 0.0263 | Val loss: 3.5565\n","Epoch: 176/1000 | Loss: 0.0271 | Val loss: 3.4372\n","Epoch: 177/1000 | Loss: 0.0266 | Val loss: 3.3234 - *Checkpoint*\n","Epoch: 178/1000 | Loss: 0.0270 | Val loss: 3.3166 - *Checkpoint*\n","Epoch: 179/1000 | Loss: 0.0266 | Val loss: 3.5109\n","Epoch: 180/1000 | Loss: 0.0263 | Val loss: 3.5050\n","Epoch: 181/1000 | Loss: 0.0265 | Val loss: 3.4872\n","Epoch: 182/1000 | Loss: 0.0261 | Val loss: 3.4304\n","Epoch: 183/1000 | Loss: 0.0265 | Val loss: 3.3522\n","Epoch: 184/1000 | Loss: 0.0261 | Val loss: 3.5601\n","Epoch: 185/1000 | Loss: 0.0262 | Val loss: 3.3036 - *Checkpoint*\n","Epoch: 186/1000 | Loss: 0.0266 | Val loss: 3.3797\n","Epoch: 187/1000 | Loss: 0.0259 | Val loss: 3.3418\n","Epoch: 188/1000 | Loss: 0.0265 | Val loss: 3.3337\n","Epoch: 189/1000 | Loss: 0.0258 | Val loss: 3.2989 - *Checkpoint*\n","Epoch: 190/1000 | Loss: 0.0260 | Val loss: 3.4102\n","Epoch: 191/1000 | Loss: 0.0266 | Val loss: 3.4029\n","Epoch: 192/1000 | Loss: 0.0257 | Val loss: 3.3661\n","Epoch: 193/1000 | Loss: 0.0258 | Val loss: 3.5836\n","Epoch: 194/1000 | Loss: 0.0257 | Val loss: 3.4050\n","Epoch: 195/1000 | Loss: 0.0261 | Val loss: 3.3562\n","Epoch: 196/1000 | Loss: 0.0259 | Val loss: 3.5946\n","Epoch: 197/1000 | Loss: 0.0256 | Val loss: 3.4000\n","Epoch: 198/1000 | Loss: 0.0260 | Val loss: 3.2928 - *Checkpoint*\n","Epoch: 199/1000 | Loss: 0.0259 | Val loss: 3.6687\n","Epoch: 200/1000 | Loss: 0.0258 | Val loss: 3.5328\n","Epoch: 201/1000 | Loss: 0.0255 | Val loss: 3.3965\n","Epoch: 202/1000 | Loss: 0.0261 | Val loss: 3.3520\n","Epoch: 203/1000 | Loss: 0.0255 | Val loss: 3.2594 - *Checkpoint*\n","Epoch: 204/1000 | Loss: 0.0253 | Val loss: 3.5874\n","Epoch: 205/1000 | Loss: 0.0258 | Val loss: 3.2701\n","Epoch: 206/1000 | Loss: 0.0258 | Val loss: 3.5201\n","Epoch: 207/1000 | Loss: 0.0254 | Val loss: 3.2553 - *Checkpoint*\n","Epoch: 208/1000 | Loss: 0.0258 | Val loss: 3.3974\n","Epoch: 209/1000 | Loss: 0.0254 | Val loss: 3.5462\n","Epoch: 210/1000 | Loss: 0.0258 | Val loss: 3.5527\n","Epoch: 211/1000 | Loss: 0.0259 | Val loss: 3.5701\n","Epoch: 212/1000 | Loss: 0.0249 | Val loss: 3.3501\n","Epoch: 213/1000 | Loss: 0.0252 | Val loss: 3.3601\n","Epoch: 214/1000 | Loss: 0.0255 | Val loss: 3.3925\n","Epoch: 215/1000 | Loss: 0.0254 | Val loss: 3.3965\n","Epoch: 216/1000 | Loss: 0.0249 | Val loss: 3.6158\n","Epoch: 217/1000 | Loss: 0.0252 | Val loss: 3.2754\n","Epoch: 218/1000 | Loss: 0.0251 | Val loss: 3.2195 - *Checkpoint*\n","Epoch: 219/1000 | Loss: 0.0247 | Val loss: 3.4104\n","Epoch: 220/1000 | Loss: 0.0248 | Val loss: 3.2569\n","Epoch: 221/1000 | Loss: 0.0253 | Val loss: 3.3399\n","Epoch: 222/1000 | Loss: 0.0246 | Val loss: 3.2414\n","Epoch: 223/1000 | Loss: 0.0248 | Val loss: 3.4098\n","Epoch: 224/1000 | Loss: 0.0245 | Val loss: 3.2997\n","Epoch: 225/1000 | Loss: 0.0252 | Val loss: 3.2562\n","Epoch: 226/1000 | Loss: 0.0246 | Val loss: 3.3893\n","Epoch: 227/1000 | Loss: 0.0247 | Val loss: 3.2836\n","Epoch: 228/1000 | Loss: 0.0253 | Val loss: 3.5065\n","Epoch: 229/1000 | Loss: 0.0245 | Val loss: 3.3922\n","Epoch: 230/1000 | Loss: 0.0246 | Val loss: 3.5338\n","Epoch: 231/1000 | Loss: 0.0246 | Val loss: 3.2932\n","Epoch: 232/1000 | Loss: 0.0247 | Val loss: 3.2310\n","Epoch: 233/1000 | Loss: 0.0244 | Val loss: 3.2809\n","Epoch: 234/1000 | Loss: 0.0245 | Val loss: 3.5486\n","Epoch: 235/1000 | Loss: 0.0246 | Val loss: 3.2589\n","Epoch: 236/1000 | Loss: 0.0248 | Val loss: 3.4673\n","Epoch: 237/1000 | Loss: 0.0243 | Val loss: 3.2798\n","Epoch: 238/1000 | Loss: 0.0245 | Val loss: 3.4560\n","Epoch 00239: reducing learning rate of group 0 to 3.3000e-04.\n","Epoch: 239/1000 | Loss: 0.0241 | Val loss: 3.3870\n","Epoch: 240/1000 | Loss: 0.0232 | Val loss: 3.2464\n","Epoch: 241/1000 | Loss: 0.0231 | Val loss: 3.3212\n","Epoch: 242/1000 | Loss: 0.0231 | Val loss: 3.3341\n","Epoch: 243/1000 | Loss: 0.0231 | Val loss: 3.2151 - *Checkpoint*\n","Epoch: 244/1000 | Loss: 0.0231 | Val loss: 3.2227\n","Epoch: 245/1000 | Loss: 0.0231 | Val loss: 3.2833\n","Epoch: 246/1000 | Loss: 0.0231 | Val loss: 3.3180\n","Epoch: 247/1000 | Loss: 0.0231 | Val loss: 3.2698\n","Epoch: 248/1000 | Loss: 0.0231 | Val loss: 3.2210\n","Epoch: 249/1000 | Loss: 0.0230 | Val loss: 3.3221\n","Epoch: 250/1000 | Loss: 0.0230 | Val loss: 3.2473\n","Epoch: 251/1000 | Loss: 0.0232 | Val loss: 3.4531\n","Epoch: 252/1000 | Loss: 0.0231 | Val loss: 3.3225\n","Epoch: 253/1000 | Loss: 0.0230 | Val loss: 3.2862\n","Epoch: 254/1000 | Loss: 0.0230 | Val loss: 3.2405\n","Epoch: 255/1000 | Loss: 0.0230 | Val loss: 3.2497\n","Epoch: 256/1000 | Loss: 0.0230 | Val loss: 3.2979\n","Epoch: 257/1000 | Loss: 0.0230 | Val loss: 3.3358\n","Epoch: 258/1000 | Loss: 0.0230 | Val loss: 3.2007 - *Checkpoint*\n","Epoch: 259/1000 | Loss: 0.0230 | Val loss: 3.3932\n","Epoch: 260/1000 | Loss: 0.0230 | Val loss: 3.2552\n","Epoch: 261/1000 | Loss: 0.0229 | Val loss: 3.2256\n","Epoch: 262/1000 | Loss: 0.0230 | Val loss: 3.2137\n","Epoch: 263/1000 | Loss: 0.0229 | Val loss: 3.2538\n","Epoch: 264/1000 | Loss: 0.0229 | Val loss: 3.2326\n","Epoch: 265/1000 | Loss: 0.0229 | Val loss: 3.2461\n","Epoch: 266/1000 | Loss: 0.0229 | Val loss: 3.2092\n","Epoch: 267/1000 | Loss: 0.0230 | Val loss: 3.2493\n","Epoch: 268/1000 | Loss: 0.0228 | Val loss: 3.2650\n","Epoch: 269/1000 | Loss: 0.0229 | Val loss: 3.4371\n","Epoch: 270/1000 | Loss: 0.0231 | Val loss: 3.4204\n","Epoch: 271/1000 | Loss: 0.0231 | Val loss: 3.2306\n","Epoch: 272/1000 | Loss: 0.0228 | Val loss: 3.3110\n","Epoch: 273/1000 | Loss: 0.0228 | Val loss: 3.2415\n","Epoch: 274/1000 | Loss: 0.0228 | Val loss: 3.3507\n","Epoch: 275/1000 | Loss: 0.0228 | Val loss: 3.3841\n","Epoch: 276/1000 | Loss: 0.0228 | Val loss: 3.2551\n","Epoch: 277/1000 | Loss: 0.0228 | Val loss: 3.2525\n","Epoch: 278/1000 | Loss: 0.0228 | Val loss: 3.2130\n","Epoch 00279: reducing learning rate of group 0 to 1.0890e-04.\n","Epoch: 279/1000 | Loss: 0.0228 | Val loss: 3.2896\n","Epoch: 280/1000 | Loss: 0.0224 | Val loss: 3.2167\n","Epoch: 281/1000 | Loss: 0.0224 | Val loss: 3.1829 - *Checkpoint*\n","Epoch: 282/1000 | Loss: 0.0224 | Val loss: 3.2622\n","Epoch: 283/1000 | Loss: 0.0224 | Val loss: 3.2302\n","Epoch: 284/1000 | Loss: 0.0224 | Val loss: 3.2237\n","Epoch: 285/1000 | Loss: 0.0224 | Val loss: 3.2208\n","Epoch: 286/1000 | Loss: 0.0224 | Val loss: 3.2079\n","Epoch: 287/1000 | Loss: 0.0223 | Val loss: 3.2495\n","Epoch: 288/1000 | Loss: 0.0223 | Val loss: 3.2191\n","Epoch: 289/1000 | Loss: 0.0223 | Val loss: 3.2088\n","Epoch: 290/1000 | Loss: 0.0223 | Val loss: 3.2809\n","Epoch: 291/1000 | Loss: 0.0223 | Val loss: 3.2006\n","Epoch: 292/1000 | Loss: 0.0223 | Val loss: 3.2244\n","Epoch: 293/1000 | Loss: 0.0223 | Val loss: 3.2381\n","Epoch: 294/1000 | Loss: 0.0224 | Val loss: 3.2971\n","Epoch: 295/1000 | Loss: 0.0224 | Val loss: 3.2741\n","Epoch: 296/1000 | Loss: 0.0223 | Val loss: 3.1965\n","Epoch: 297/1000 | Loss: 0.0223 | Val loss: 3.1842\n","Epoch: 298/1000 | Loss: 0.0223 | Val loss: 3.2597\n","Epoch: 299/1000 | Loss: 0.0223 | Val loss: 3.2195\n","Epoch: 300/1000 | Loss: 0.0223 | Val loss: 3.2005\n","Epoch: 301/1000 | Loss: 0.0223 | Val loss: 3.2426\n","Epoch 00302: reducing learning rate of group 0 to 3.5937e-05.\n","Epoch: 302/1000 | Loss: 0.0223 | Val loss: 3.2381\n","Epoch: 303/1000 | Loss: 0.0222 | Val loss: 3.2083\n","Epoch: 304/1000 | Loss: 0.0222 | Val loss: 3.2123\n","Epoch: 305/1000 | Loss: 0.0222 | Val loss: 3.2484\n","Epoch: 306/1000 | Loss: 0.0222 | Val loss: 3.2265\n","Epoch: 307/1000 | Loss: 0.0221 | Val loss: 3.2468\n","Epoch: 308/1000 | Loss: 0.0221 | Val loss: 3.1914\n","Epoch: 309/1000 | Loss: 0.0221 | Val loss: 3.2233\n","Epoch: 310/1000 | Loss: 0.0222 | Val loss: 3.2048\n","Epoch: 311/1000 | Loss: 0.0222 | Val loss: 3.2403\n","Epoch: 312/1000 | Loss: 0.0222 | Val loss: 3.2487\n","\n","Early stopping applied at epoch 312.\n"]}]},{"cell_type":"markdown","source":["# Inference"],"metadata":{"id":"aia7hgYg4JWm"}},{"cell_type":"code","source":["checkpoint = torch.load(\"model.pth\")\n","model.load_state_dict(checkpoint['model'])\n","optimizer.load_state_dict(checkpoint['optimizer'])\n","\n","X_enc = torch.tensor(X_test_3D_enc.copy(), dtype=torch.float32).to(device)\n","X_time_enc = torch.tensor(X_test_3D_enc[:, :, TIME_POS].copy(), dtype=torch.float32).to(device)\n","X_dec = torch.tensor(X_test_3D_dec.copy(), dtype=torch.float32).to(device)\n","X_time_dec = torch.tensor(X_test_3D_dec[:, :, TIME_POS].copy(), dtype=torch.float32).to(device)\n","\n","test_dataset = LoadDataset(X_3D_enc=X_enc, \n","                           X_3D_time_enc=X_time_enc,\n","                           X_3D_dec=X_dec, \n","                           X_3D_time_dec=X_time_dec,\n","                           y=y_test.copy())\n","test_dataloader = DataLoader(dataset=test_dataset, \n","                             batch_size=BATCH_SIZE,\n","                             shuffle=False)\n","\n","model.eval()\n","with torch.inference_mode():\n","  for batch, (X_enc, X_time_enc, X_dec, X_time_dec, Y) in enumerate(test_dataloader):\n","    X_enc, X_time_enc, X_dec, X_time_dec, Y = X_enc.to(device), X_time_enc.to(device), X_dec.to(device), X_time_dec.to(device), Y.to(device)\n","    test_batch_preds = model(x_enc=X_enc, x_mark_enc=X_time_enc, x_dec=X_dec, x_mark_dec=X_time_dec).squeeze()\n","    if batch == 0:\n","      test_preds_scaled = test_batch_preds\n","    else:\n","      test_preds_scaled = torch.cat((test_preds_scaled, test_batch_preds), dim=0)\n","\n","print(test_preds_scaled.shape)"],"metadata":{"id":"PFvF3h-7cXg3","executionInfo":{"status":"ok","timestamp":1684953023703,"user_tz":-180,"elapsed":546,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7f061131-0676-4215-c483-d133eab55f75"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-91a87307b89a>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  self.X_enc = torch.tensor(X_3D_enc, dtype=torch.float32)\n","<ipython-input-7-91a87307b89a>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  self.X_time_enc = torch.tensor(X_3D_time_enc, dtype=torch.float32)\n","<ipython-input-7-91a87307b89a>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  self.X_dec = torch.tensor(X_3D_dec, dtype=torch.float32)\n","<ipython-input-7-91a87307b89a>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  self.X_time_dec = torch.tensor(X_3D_time_dec, dtype=torch.float32)\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([8737, 24])\n"]}]},{"cell_type":"code","source":["test_preds_scaled = test_preds_scaled.to('cpu').numpy()\n","y_test_scaled = y_test.squeeze()\n","\n","test_preds = np.zeros(test_preds_scaled.shape)\n","for i in range(test_preds.shape[1]):\n","  test_preds[:, i] = test_preds_scaled[:, i] * scaler.max_abs_[TARGET_POS]\n","\n","y_test = np.zeros(y_test_scaled.shape)\n","for i in range(test_preds.shape[1]):\n","  y_test[:, i] = y_test_scaled[:, i] * scaler.max_abs_[TARGET_POS] "],"metadata":{"id":"-Ur5QzSDNzq2","executionInfo":{"status":"ok","timestamp":1684953023704,"user_tz":-180,"elapsed":4,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["mape_list = list()\n","step_results_dict = {}\n","for step in range(STEPS_FORWARD):\n","  step_results_df = pd.DataFrame(\n","      {\n","          \"real\": y_test[:, step],\n","          \"predictions\": test_preds[:, step]\n","      }\n","  )\n","  step_results_df['abs_error'] = abs(step_results_df['real'] - step_results_df['predictions'])\n","  step_results_df['ape'] = np.where(step_results_df['real'] == 0, np.NaN, 100 * step_results_df['abs_error']/step_results_df['real'])\n","  step_mape = step_results_df['ape'].mean()\n","  mape_list.append(step_mape)\n","  print(f\"Step {step} -> MAPE = {step_mape}\")\n","\n","  step_results_dict[step] = step_results_df\n","mape = np.array(mape_list).mean()\n","print(f\"\\nMAPE = {mape}\")"],"metadata":{"id":"EJSskMCYKBb5","executionInfo":{"status":"ok","timestamp":1684953023704,"user_tz":-180,"elapsed":3,"user":{"displayName":"Achil Pal","userId":"04608072963754583562"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"34b6c5e8-fa71-47f6-8695-7cf073935ac8"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Step 0 -> MAPE = 2.729022637940422\n","Step 1 -> MAPE = 2.7720945149979106\n","Step 2 -> MAPE = 2.898093454732678\n","Step 3 -> MAPE = 2.956590204828603\n","Step 4 -> MAPE = 3.000225872679718\n","Step 5 -> MAPE = 3.057659713091322\n","Step 6 -> MAPE = 3.1544333045111115\n","Step 7 -> MAPE = 3.2269032648513645\n","Step 8 -> MAPE = 3.1734984255614584\n","Step 9 -> MAPE = 3.19854421566061\n","Step 10 -> MAPE = 3.2195345913018167\n","Step 11 -> MAPE = 3.319253351571106\n","Step 12 -> MAPE = 3.3234515303640584\n","Step 13 -> MAPE = 3.5262040928076024\n","Step 14 -> MAPE = 3.5979370098943653\n","Step 15 -> MAPE = 3.600307698343608\n","Step 16 -> MAPE = 3.6073483585332204\n","Step 17 -> MAPE = 3.698461838424507\n","Step 18 -> MAPE = 3.786883661349547\n","Step 19 -> MAPE = 3.8202653569921203\n","Step 20 -> MAPE = 3.781110481428837\n","Step 21 -> MAPE = 3.7662384423941986\n","Step 22 -> MAPE = 3.8075315420324127\n","Step 23 -> MAPE = 3.836756191207882\n","\n","MAPE = 3.369097906479187\n"]}]}]}