{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","toc_visible":true,"gpuType":"V100","authorship_tag":"ABX9TyMmrMLGu8mqzW5dCeNDqUpq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b87d2795ed8741ee936b35517c74856f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30cd1aab593148a39a4a638c69c19d45","IPY_MODEL_a0b1e302f4de4046920d1e77ed65caec","IPY_MODEL_fe098c5f639d461db38f81f5b24d66a4"],"layout":"IPY_MODEL_2258ef21397d47219619206432980bd0"}},"30cd1aab593148a39a4a638c69c19d45":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a86c9e09624b4b3dab7cf427467994ec","placeholder":"​","style":"IPY_MODEL_0259d8443c924a4bb566a1f73e46649f","value":" 16%"}},"a0b1e302f4de4046920d1e77ed65caec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_d58d241dfc6b4bbbaae5bd66bf519dbe","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c0096800e2ac4476baf719a5ac253d41","value":311}},"fe098c5f639d461db38f81f5b24d66a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_208aabebfd6c4f1187bd17b11573259d","placeholder":"​","style":"IPY_MODEL_dfeecfc131c547718e3738f0716c612f","value":" 311/2000 [02:55&lt;15:51,  1.77it/s]"}},"2258ef21397d47219619206432980bd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a86c9e09624b4b3dab7cf427467994ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0259d8443c924a4bb566a1f73e46649f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d58d241dfc6b4bbbaae5bd66bf519dbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0096800e2ac4476baf719a5ac253d41":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"208aabebfd6c4f1187bd17b11573259d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfeecfc131c547718e3738f0716c612f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install torchmetrics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ueHGkcxOOpF3","executionInfo":{"status":"ok","timestamp":1697215122970,"user_tz":-180,"elapsed":6286,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}},"outputId":"cdaacf03-9a7c-4720-dbf4-43528c9af3c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchmetrics\n","  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (17.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n","Installing collected packages: lightning-utilities, torchmetrics\n","Successfully installed lightning-utilities-0.9.0 torchmetrics-1.2.0\n"]}]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"mnqvk5BO_Qh8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-aAf6BX89Tvo"},"outputs":[],"source":["import math\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","from google.colab import files\n","from tqdm.auto import tqdm\n","from torchmetrics import MeanAbsolutePercentageError\n","from datetime import datetime"]},{"cell_type":"markdown","source":["# Classes + Helpers"],"metadata":{"id":"ene0f0Uz_SYb"}},{"cell_type":"markdown","source":["## Data processing"],"metadata":{"id":"5VrJ6nvMbK8w"}},{"cell_type":"code","source":["def scale_data(load_df,\n","               start_train_date,\n","               end_val_date,\n","               start_test_date,\n","               end_test_date):\n","\n","  train_val_df = load_df[(load_df.index >= start_train_date) &\n","                        (load_df.index <= end_val_date)]\n","  test_df = load_df[(load_df.index >= start_test_date) &\n","                    (load_df.index <= end_test_date)]\n","\n","  scaler = MinMaxScaler()\n","  train_val_scaled = scaler.fit_transform(train_val_df)\n","  train_val_df_scaled = pd.DataFrame(train_val_scaled,\n","                                    columns=train_val_df.columns,\n","                                    index=train_val_df.index)\n","  test_scaled = scaler.transform(test_df)\n","  test_df_scaled = pd.DataFrame(test_scaled,\n","                                columns=test_df.columns,\n","                                index=test_df.index)\n","\n","  load_df_scaled = pd.concat([train_val_df_scaled, test_df_scaled], axis=0)\n","\n","  return load_df_scaled, scaler"],"metadata":{"id":"20DCtZ_EJ8ti"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reframing(X_df, Y_df, n_backwards=1, skip_steps_forward=0, n_forwards=1):\n","\n","  feat_cols, feat_names = [], []\n","  # iterate through all columns\n","  for col_index, col_name in enumerate(X_df.columns):\n","    series = X_df[col_name].copy()\n","    # input sequence (t, t-1, ... ,t-(n_backwards+1) )\n","    for b in range(n_backwards):\n","      feat_cols.append(series.shift(b))\n","      feat_names.append(f'{col_name}_(t-{b})')\n","\n","  # put it all together\n","  X = pd.concat(feat_cols, axis=1)\n","  X.columns = feat_names\n","  # drop rows with NaN values\n","  X.dropna(inplace=True)\n","  X_index = X.index\n","\n","  # forecast sequence (t + SKIP_STEPS_FORWARD + 1, ... , t + n_forwards)\n","  series = Y_df\n","  target_cols, target_names = [], []\n","  for f in range(skip_steps_forward + 1, n_forwards):\n","    target_cols.append(Y_df.shift(-f))\n","    if f == 0:\n","      target_names.append(f'{Y_df.name}_t')\n","    else:\n","      target_names.append(f'{Y_df.name}_(t+{f})')\n","\n","  # put it all together\n","  Y = pd.concat(target_cols, axis=1)\n","  Y.columns = target_names\n","  # drop rows with NaN values\n","  Y.dropna(inplace=True)\n","  Y_index = Y.index\n","\n","  return X, X_index, Y, Y_index\n","\n","\n","def reframe_data(load_df_scaled,\n","                 target,\n","                 days_back,\n","                 last_step_forward,\n","                 last_step_back,\n","                 skip_steps_forward):\n","\n","  time_weather_cols = load_df_scaled.drop(TARGET, axis=1).columns\n","\n","  # shift future values\n","  for col in time_weather_cols:\n","    load_df_scaled[col + f'_(t+{LAST_STEP_FORWARD})'] = load_df_scaled[col].shift(-LAST_STEP_FORWARD)\n","    load_df_scaled.drop(col, axis=1, inplace=True)\n","\n","  X_orig, X_index, Y_orig, Y_index = reframing(load_df_scaled,\n","                                                load_df_scaled[TARGET],\n","                                                n_backwards=LAST_STEP_BACK,\n","                                                skip_steps_forward=SKIP_STEPS_FORWARD,\n","                                                n_forwards=LAST_STEP_FORWARD+1)\n","  common_index = X_index.intersection(Y_index)\n","  X_df = X_orig.loc[common_index]\n","  Y_df = Y_orig.loc[common_index]\n","\n","  load_df_scaled_reframed = pd.concat([X_df, Y_df], axis=1)\n","\n","  return load_df_scaled_reframed"],"metadata":{"id":"lh1YUHYrKTEQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create3Dinput(df, last_step_back):\n","  N, D = df.shape\n","  D = int(D/last_step_back)\n","  arr_3d = np.zeros((N, last_step_back, D))\n","  for i in range(D):\n","    arr_3d[:, :, i] = df.iloc[:, i*last_step_back:(i+1)*last_step_back].values\n","  print(arr_3d.shape)\n","  return arr_3d\n","\n","\n","def split_data(load_df_scaled_reframed,\n","               start_train_date,\n","               end_train_date,\n","               start_val_date,\n","               end_val_date,\n","               start_test_date,\n","               end_test_date,\n","               steps_forward,\n","               last_step_back):\n","\n","  load_train_df_scaled_reframed = load_df_scaled_reframed[(load_df_scaled_reframed.index >= start_train_date) &\n","                                                          (load_df_scaled_reframed.index <= end_train_date)]\n","\n","  load_val_df_scaled_reframed = load_df_scaled_reframed[(load_df_scaled_reframed.index >= start_val_date) &\n","                                                        (load_df_scaled_reframed.index <= end_val_date)]\n","\n","  load_test_df_scaled_reframed = load_df_scaled_reframed[(load_df_scaled_reframed.index >= start_test_date) &\n","                                                        (load_df_scaled_reframed.index <= end_test_date)]\n","\n","  # load_train_df_scaled_reframed = shuffle(load_train_df_scaled_reframed)\n","\n","  X_train_df = load_train_df_scaled_reframed.iloc[:, :-steps_forward]\n","  y_train_df = load_train_df_scaled_reframed.iloc[:, -steps_forward:]\n","\n","  X_val_df = load_val_df_scaled_reframed.iloc[:, :-steps_forward]\n","  y_val_df = load_val_df_scaled_reframed.iloc[:, -steps_forward:]\n","\n","  X_test_df = load_test_df_scaled_reframed.iloc[:, :-steps_forward]\n","  y_test_df = load_test_df_scaled_reframed.iloc[:, -steps_forward:]\n","\n","  X_train_3D = create3Dinput(X_train_df, last_step_back)\n","  X_val_3D = create3Dinput(X_val_df, last_step_back)\n","  X_test_3D = create3Dinput(X_test_df, last_step_back)\n","\n","  return X_train_3D, X_val_3D, X_test_3D, y_train_df, y_val_df, y_test_df"],"metadata":{"id":"V-8n4sGJKgGw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LoadDataset(Dataset):\n","  def __init__(self, X_3D, y_df):\n","    self.X = torch.tensor(X_3D, dtype=torch.float32)\n","    self.y = torch.tensor(y_df.values, dtype=torch.float32)\n","\n","  def __len__(self):\n","    return len(self.y)\n","\n","  def __getitem__(self, idx):\n","    return self.X[idx], self.y[idx]"],"metadata":{"id":"YHTGg8XFOmJq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transformer - Encoder"],"metadata":{"id":"04ysdCZGbG_H"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  def __init__(self, d_k, d_model, n_heads):\n","    super().__init__()\n","\n","    # Assume d_v = d_k\n","    self.d_k = d_k\n","    self.n_heads = n_heads\n","\n","    self.key = nn.Linear(d_model, d_k * n_heads)\n","    self.query = nn.Linear(d_model, d_k * n_heads)\n","    self.value = nn.Linear(d_model, d_k * n_heads)\n","\n","    # final linear layer\n","    self.fc = nn.Linear(d_k * n_heads, d_model)\n","\n","  def forward(self, q, k, v, mask=None):\n","    # h -> Number of attention heads\n","    q = self.query(q)   # [N, T, d_model] --> [N, T, h*d_k]\n","    k = self.key(k)     # [N, T, d_model] --> [N, T, h*d_k]\n","    v = self.value(v)   # [N, T, d_model] --> [N, T, h*d_v]\n","\n","    N = q.shape[0]    # batch size\n","    T = q.shape[1]    # sequence length\n","\n","    # make the following change in shape:\n","    # [N, T, h, d_k (or d_v)] --> [N, h, T, d_k (or d_v)]\n","    # in order for multiplication to work properly\n","    q = q.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n","    k = k.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n","    v = v.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n","\n","    # compute attention weights\n","    # [N, h, T, d_k] x [N, h, d_k, T] --> [N, h, T, T]\n","    attention_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n","    if mask is not None:\n","      attention_scores = attention_scores.masked_fill(\n","          mask[:, None, None, :] == 0, float('-inf')\n","      )\n","    attention_weights = F.softmax(attention_scores, dim=-1)\n","\n","    # compute attention-weighted values\n","    # [N, h, T, T] x [N, h, T, d_k] --> [N, h, T, d_k]\n","    A = attention_weights @ v\n","\n","    # reshape it back before final linear layer\n","    A = A.transpose(1, 2)   # [N, T, h, d_k]\n","    A = A.contiguous().view(N, T, self.d_k * self.n_heads)    # [N, T, h*d_k]\n","\n","    # projection\n","    return self.fc(A)"],"metadata":{"id":"KPQ_Wu5z_VWU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","  def __init__(self, d_k, d_model, n_heads, dropout_prob=0.1):\n","    super().__init__()\n","\n","    self.ln1 = nn.LayerNorm(d_model)\n","    self.ln2 = nn.LayerNorm(d_model)\n","    self.mha = MultiHeadAttention(d_k, d_model, n_heads)\n","    self.ann = nn.Sequential(\n","        nn.Linear(d_model, 4 * d_model),\n","        nn.GELU(),\n","        nn.Linear(4 * d_model, d_model),\n","        nn.GELU(),\n","        nn.Dropout(p=dropout_prob),\n","    )\n","    self.dropout = nn.Dropout(p=dropout_prob)\n","\n","  def forward(self, x, mask=None):\n","    x = self.ln1(x + self.mha(x, x, x, mask))\n","    x = self.ln2(x + self.ann(x))\n","    x = self.dropout(x)\n","    return x"],"metadata":{"id":"mWpFelw8HIi5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","  def __init__(self, d_model, max_len=1024, dropout_prob=0.1):\n","    super().__init__()\n","    self.dropout = nn.Dropout(p=dropout_prob)\n","\n","    position = torch.arange(max_len).unsqueeze(1)\n","    exp_term = torch.arange(0, d_model, 2)\n","    div_term = torch.exp(exp_term * (-math.log(10000.0) / d_model))\n","    pe = torch.zeros(1, max_len, d_model)\n","    pe[0, :, 0::2] = torch.sin(position * div_term)\n","    pe[0, :, 1::2] = torch.cos(position * div_term)\n","    self.register_buffer('pe', pe)\n","\n","  def forward(self, x):\n","    # x --> [N, T, D]\n","    x = x + self.pe[:, :x.size(1), :]\n","    return self.dropout(x)"],"metadata":{"id":"ZSE02BEKJMYN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","  def __init__(self,\n","               D,\n","               max_len,\n","               d_k,\n","               d_model,\n","               n_heads,\n","               n_layers,\n","               output_units,\n","               dropout_prob):\n","    super().__init__()\n","\n","    # self.conv1d = nn.Conv1d(in_channels=D,\n","    #                         out_channels=d_model,\n","    #                         kernel_size=1)\n","    self.linear = nn.Linear(D, d_model)\n","    self.relu = nn.ReLU()\n","    self.pos_encoding = PositionalEncoding(d_model,\n","                                           max_len,\n","                                           dropout_prob)\n","    transformer_blocks = [\n","        TransformerBlock(\n","            d_k,\n","            d_model,\n","            n_heads,\n","            dropout_prob) for _ in range(n_layers)]\n","    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n","    self.ln = nn.LayerNorm(d_model)\n","    self.fc = nn.Linear(d_model, output_units)\n","\n","  def forward(self, x, mask=None):\n","    # x = self.conv1d(x).transpose(1, 2)\n","    x = self.relu(self.linear(x))\n","    x = self.pos_encoding(x)\n","    for block in self.transformer_blocks:\n","      x = block(x, mask)\n","\n","    # many-to-one (x --> [N, T, D])\n","    x = x[:, 0, :]\n","\n","    x = self.ln(x)\n","    x = self.fc(x)\n","    return x"],"metadata":{"id":"3LoBGSruLv7_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training and Evaluation"],"metadata":{"id":"xF7JUjfxbfyE"}},{"cell_type":"code","source":["def mape(y_preds, y_true):\n","  epsilon = 1.17e-06\n","  abs_diff = torch.abs(y_preds - y_true)\n","  abs_per_error = abs_diff / torch.clamp(torch.abs(y_true), min=epsilon)\n","  mape = torch.sum(abs_per_error) / y_true.numel()\n","\n","  return mape\n","\n","def mape_mod(y_preds, y_true):\n","  epsilon = 1.17e-06\n","  abs_diff_2 = torch.square(torch.abs(y_preds - y_true))\n","  abs_per_error = abs_diff_2 / torch.clamp(torch.abs(y_true), min=epsilon)\n","  mape = torch.sum(abs_per_error) / y_true.numel()\n","\n","  return mape\n","\n","def loss_fn(y_preds, y_true, device):\n","  MAPE = mape(y_preds, y_true)\n","  loss = MAPE * (1 + 0.4 * torch.max(torch.tensor([0, MAPE - 2])))\n","  return loss.to(device).requires_grad_(True)\n","\n","# def loss_fn(y_preds, y_true, device):\n","#   return torch.exp(2 * torch.square(mape(y_preds, y_true))).to(device).requires_grad_(True)\n","\n","def loss_fn(y_preds, y_true, device):\n","  loss = torch.exp(mape_mod(y_preds, y_true) - 3)\n","  return loss.requires_grad_(True)\n","\n","# def loss_fn(y_preds, y_true, device, a, b, c):\n","  # MAPE = mape(y_preds, y_true)\n","  # loss = MAPE * (torch.abs(a) + torch.abs(b) * torch.max(torch.tensor([0, MAPE - c])))\n","  # return loss.to(device).requires_grad_(True)\n","\n","def train_step(model,\n","               dataloader,\n","               optimizer,\n","               device,):\n","\n","  model.train()\n","  loss = 0\n","  for batch, (X, y) in enumerate(dataloader):\n","    X, y = X.to(device), y.to(device)\n","    y_preds = model(X).squeeze().to(device)\n","    batch_loss = loss_fn(y_preds, y, device)\n","    loss += batch_loss.item()\n","    optimizer.zero_grad()\n","    batch_loss.backward()\n","    optimizer.step()\n","\n","  loss /= len(dataloader)\n","  return loss\n","\n","def val_step(model, dataloader, device):\n","  model.eval()\n","  val_loss = 0\n","  with torch.inference_mode():\n","    for batch, (X, y) in enumerate(dataloader):\n","      X, y = X.to(device), y.to(device)\n","      val_preds = model(X).squeeze().to(device)\n","\n","      y_preds_unscaled = scaler.data_min_[TARGET_POS] + val_preds * (scaler.data_max_[TARGET_POS] - scaler.data_min_[TARGET_POS])\n","      y_true_unscaled  = scaler.data_min_[TARGET_POS] + y * (scaler.data_max_[TARGET_POS] - scaler.data_min_[TARGET_POS])\n","\n","      batch_loss = torch.tensor(100) * mape(y_preds_unscaled, y_true_unscaled) # loss_fn(val_preds, y, device)\n","      val_loss += batch_loss.item()\n","\n","  val_loss /= len(dataloader)\n","  return val_loss\n","\n","def train(model,\n","          train_dataloader,\n","          val_dataloader,\n","          optimizer,\n","          epochs,\n","          patience,\n","          device,\n","          scheduler,\n","          path):\n","\n","  results = {\n","      \"loss\": [],\n","      \"val_loss\": []\n","  }\n","\n","  for epoch in tqdm(range(epochs)):\n","    flag = 0\n","    loss = train_step(model=model,\n","                      dataloader=train_dataloader,\n","                      optimizer=optimizer,\n","                      device=device,)\n","                      # a=model.a,\n","                      # b=model.b,\n","                      # c=model.c)\n","\n","    val_loss = val_step(model=model,\n","                        dataloader=val_dataloader,\n","                        device=device)\n","\n","    scheduler.step(val_loss)\n","\n","    results['loss'].append(loss)\n","    results['val_loss'].append(val_loss)\n","    if epoch == 0:\n","      best_val_loss = val_loss\n","      best_epoch = -1\n","      checkpoint(model, optimizer, path)\n","      flag = 1\n","      print(f\"Epoch: {epoch+1}/{epochs} | Loss: {loss:.4f} | Val loss: {val_loss:.4f} - *Checkpoint*\")\n","    else:\n","      if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        best_epoch = epoch\n","        checkpoint(model, optimizer, path)\n","        flag = 1\n","        print(f\"Epoch: {epoch+1}/{epochs} | Loss: {loss:.4f} | Val loss: {val_loss:.4f} - *Checkpoint*\")\n","      elif epoch - best_epoch > patience:\n","        print(f\"\\nEarly stopping applied at epoch {epoch}.\")\n","        break\n","    if flag == 0:\n","      print(f\"Epoch: {epoch+1}/{epochs} | Loss: {loss:.4f} | Val loss: {val_loss:.4f}\")\n","\n","  return results\n","\n","def checkpoint(model, optimizer, filepath):\n","  torch.save({\n","    \"optimizer\": optimizer.state_dict(),\n","    \"model\": model.state_dict()\n","  }, filepath)"],"metadata":{"id":"s2egVAsMZW6Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MAIN"],"metadata":{"id":"waEWWevba5jX"}},{"cell_type":"code","source":["DAYS_BACK = 2\n","DAYS_TO_SKIP = 10\n","STEPS_FORWARD = 24    # 1 day\n","SKIP_STEPS_FORWARD = 24 * DAYS_TO_SKIP\n","LAST_STEP_FORWARD = STEPS_FORWARD + SKIP_STEPS_FORWARD\n","LAST_STEP_BACK = 24 * DAYS_BACK\n","\n","# keep 1 year for testing\n","START_TEST_DATE = pd.to_datetime('2018-01-01') - pd.to_timedelta(LAST_STEP_FORWARD, 'h')\n","END_TEST_DATE = START_TEST_DATE + pd.DateOffset(years=1)\n","\n","END_VAL_DATE = START_TEST_DATE - pd.to_timedelta(1, 'h')\n","START_VAL_DATE = pd.to_datetime('2017-01-01') - pd.to_timedelta(LAST_STEP_FORWARD, 'h')\n","\n","START_TRAIN_DATE = pd.to_datetime('2016-01-01')\n","END_TRAIN_DATE = START_VAL_DATE - pd.to_timedelta(1, 'h')\n","\n","TARGET = \"TOTAL_CONS\"\n","\n","BATCH_SIZE = 2048\n","\n","print(f\"Train from {START_TRAIN_DATE} to {END_TRAIN_DATE}\")\n","print(f\"Validation from {START_VAL_DATE} to {END_VAL_DATE}\")\n","print(f\"Test from {START_TEST_DATE} to {END_TEST_DATE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cy3AXiz2yQjN","executionInfo":{"status":"ok","timestamp":1697215173087,"user_tz":-180,"elapsed":603,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}},"outputId":"1c61760d-d932-4074-9825-a0cdcee79d83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train from 2016-01-01 00:00:00 to 2016-12-20 23:00:00\n","Validation from 2016-12-21 00:00:00 to 2017-12-20 23:00:00\n","Test from 2017-12-21 00:00:00 to 2018-12-21 00:00:00\n"]}]},{"cell_type":"code","source":["load_df = pd.read_csv(\"/content/FINAL_DATASET_2.csv\")\n","load_df.set_index(pd.to_datetime(load_df[\"Timestamp\"]), inplace=True)\n","load_df.drop(\"Timestamp\", axis=1, inplace=True)\n","\n","TARGET_POS = np.where(load_df.columns == TARGET)[0][0]\n","\n","load_df_scaled, scaler = scale_data(load_df,\n","                                    START_TRAIN_DATE,\n","                                    END_VAL_DATE,\n","                                    START_TEST_DATE,\n","                                    END_TEST_DATE)\n","\n","load_df_scaled_reframed = reframe_data(load_df_scaled,\n","                                       TARGET,\n","                                       DAYS_BACK,\n","                                       LAST_STEP_FORWARD,\n","                                       LAST_STEP_BACK,\n","                                       SKIP_STEPS_FORWARD)"],"metadata":{"id":"6cRrYOkQmUgs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load_df_scaled_reframed"],"metadata":{"id":"iKpX7rbo0vVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["load_df_scaled.iloc[0:50, TARGET_POS]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2KftL3XvEix","executionInfo":{"status":"ok","timestamp":1697215174023,"user_tz":-180,"elapsed":6,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}},"outputId":"b98d54b6-6a02-4d36-b33c-fc1af981529e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Timestamp\n","2016-01-01 00:00:00    0.486232\n","2016-01-01 01:00:00    0.419525\n","2016-01-01 02:00:00    0.396921\n","2016-01-01 03:00:00    0.345315\n","2016-01-01 04:00:00    0.300246\n","2016-01-01 05:00:00    0.286908\n","2016-01-01 06:00:00    0.293643\n","2016-01-01 07:00:00    0.307689\n","2016-01-01 08:00:00    0.344796\n","2016-01-01 09:00:00    0.432351\n","2016-01-01 10:00:00    0.534945\n","2016-01-01 11:00:00    0.624341\n","2016-01-01 12:00:00    0.640159\n","2016-01-01 13:00:00    0.578367\n","2016-01-01 14:00:00    0.465812\n","2016-01-01 15:00:00    0.501191\n","2016-01-01 16:00:00    0.479517\n","2016-01-01 17:00:00    0.538182\n","2016-01-01 18:00:00    0.642194\n","2016-01-01 19:00:00    0.662236\n","2016-01-01 20:00:00    0.653065\n","2016-01-01 21:00:00    0.623115\n","2016-01-01 22:00:00    0.562604\n","2016-01-01 23:00:00    0.514960\n","2016-01-02 00:00:00    0.429809\n","2016-01-02 01:00:00    0.351936\n","2016-01-02 02:00:00    0.342753\n","2016-01-02 03:00:00    0.325824\n","2016-01-02 04:00:00    0.293721\n","2016-01-02 05:00:00    0.292223\n","2016-01-02 06:00:00    0.291955\n","2016-01-02 07:00:00    0.319682\n","2016-01-02 08:00:00    0.364162\n","2016-01-02 09:00:00    0.462189\n","2016-01-02 10:00:00    0.564266\n","2016-01-02 11:00:00    0.609994\n","2016-01-02 12:00:00    0.635481\n","2016-01-02 13:00:00    0.626194\n","2016-01-02 14:00:00    0.564292\n","2016-01-02 15:00:00    0.576741\n","2016-01-02 16:00:00    0.573437\n","2016-01-02 17:00:00    0.630723\n","2016-01-02 18:00:00    0.720886\n","2016-01-02 19:00:00    0.725256\n","2016-01-02 20:00:00    0.706590\n","2016-01-02 21:00:00    0.659438\n","2016-01-02 22:00:00    0.577805\n","2016-01-02 23:00:00    0.518053\n","2016-01-03 00:00:00    0.435855\n","2016-01-03 01:00:00    0.351581\n","Name: TOTAL_CONS, dtype: float64"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# X_train_3D[0, :, TARGET_POS]"],"metadata":{"id":"DxTc5-13zlzt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = load_df_scaled[(load_df_scaled[TARGET] > 0.51805285) & (load_df_scaled[TARGET] < 0.51805287)].index.item()\n","idx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tg3bL-cVwwqJ","executionInfo":{"status":"ok","timestamp":1697215174024,"user_tz":-180,"elapsed":6,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}},"outputId":"7c174070-9a6a-46ee-f7ae-9c4f13c85d68"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Timestamp('2016-01-02 23:00:00')"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["load_df_scaled[load_df_scaled.index <= idx][TARGET].tail(48)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yNu0vl5AxTFP","executionInfo":{"status":"ok","timestamp":1697215174024,"user_tz":-180,"elapsed":5,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}},"outputId":"bdce5cb2-18d5-448c-c564-b6d28ad8f4bd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Timestamp\n","2016-01-01 00:00:00    0.486232\n","2016-01-01 01:00:00    0.419525\n","2016-01-01 02:00:00    0.396921\n","2016-01-01 03:00:00    0.345315\n","2016-01-01 04:00:00    0.300246\n","2016-01-01 05:00:00    0.286908\n","2016-01-01 06:00:00    0.293643\n","2016-01-01 07:00:00    0.307689\n","2016-01-01 08:00:00    0.344796\n","2016-01-01 09:00:00    0.432351\n","2016-01-01 10:00:00    0.534945\n","2016-01-01 11:00:00    0.624341\n","2016-01-01 12:00:00    0.640159\n","2016-01-01 13:00:00    0.578367\n","2016-01-01 14:00:00    0.465812\n","2016-01-01 15:00:00    0.501191\n","2016-01-01 16:00:00    0.479517\n","2016-01-01 17:00:00    0.538182\n","2016-01-01 18:00:00    0.642194\n","2016-01-01 19:00:00    0.662236\n","2016-01-01 20:00:00    0.653065\n","2016-01-01 21:00:00    0.623115\n","2016-01-01 22:00:00    0.562604\n","2016-01-01 23:00:00    0.514960\n","2016-01-02 00:00:00    0.429809\n","2016-01-02 01:00:00    0.351936\n","2016-01-02 02:00:00    0.342753\n","2016-01-02 03:00:00    0.325824\n","2016-01-02 04:00:00    0.293721\n","2016-01-02 05:00:00    0.292223\n","2016-01-02 06:00:00    0.291955\n","2016-01-02 07:00:00    0.319682\n","2016-01-02 08:00:00    0.364162\n","2016-01-02 09:00:00    0.462189\n","2016-01-02 10:00:00    0.564266\n","2016-01-02 11:00:00    0.609994\n","2016-01-02 12:00:00    0.635481\n","2016-01-02 13:00:00    0.626194\n","2016-01-02 14:00:00    0.564292\n","2016-01-02 15:00:00    0.576741\n","2016-01-02 16:00:00    0.573437\n","2016-01-02 17:00:00    0.630723\n","2016-01-02 18:00:00    0.720886\n","2016-01-02 19:00:00    0.725256\n","2016-01-02 20:00:00    0.706590\n","2016-01-02 21:00:00    0.659438\n","2016-01-02 22:00:00    0.577805\n","2016-01-02 23:00:00    0.518053\n","Name: TOTAL_CONS, dtype: float64"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["# X_train_3D[1, :, TARGET_POS]"],"metadata":{"id":"iVo6cd7fvPto"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["load_df_scaled[load_df_scaled.index > idx + pd.Timedelta(240, 'h')][TARGET].head(24)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g7a02J4lyumD","executionInfo":{"status":"ok","timestamp":1697215174024,"user_tz":-180,"elapsed":4,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}},"outputId":"b53030ca-bd16-4e79-aa80-0646b67a1fcf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Timestamp\n","2016-01-13 00:00:00    0.315948\n","2016-01-13 01:00:00    0.239922\n","2016-01-13 02:00:00    0.242758\n","2016-01-13 03:00:00    0.221896\n","2016-01-13 04:00:00    0.212520\n","2016-01-13 05:00:00    0.226675\n","2016-01-13 06:00:00    0.290323\n","2016-01-13 07:00:00    0.415938\n","2016-01-13 08:00:00    0.461947\n","2016-01-13 09:00:00    0.513777\n","2016-01-13 10:00:00    0.528301\n","2016-01-13 11:00:00    0.534051\n","2016-01-13 12:00:00    0.534743\n","2016-01-13 13:00:00    0.537886\n","2016-01-13 14:00:00    0.500903\n","2016-01-13 15:00:00    0.507191\n","2016-01-13 16:00:00    0.504135\n","2016-01-13 17:00:00    0.555458\n","2016-01-13 18:00:00    0.667843\n","2016-01-13 19:00:00    0.684129\n","2016-01-13 20:00:00    0.682373\n","2016-01-13 21:00:00    0.615970\n","2016-01-13 22:00:00    0.513415\n","2016-01-13 23:00:00    0.436524\n","Name: TOTAL_CONS, dtype: float64"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["# y_train_df.iloc[0]"],"metadata":{"id":"uqpGnhI8vayg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_3D, X_val_3D, X_test_3D, y_train_df, y_val_df, y_test_df = split_data(load_df_scaled_reframed,\n","                                                                              START_TRAIN_DATE,\n","                                                                              END_TRAIN_DATE,\n","                                                                              START_VAL_DATE,\n","                                                                              END_VAL_DATE,\n","                                                                              START_TEST_DATE,\n","                                                                              END_TEST_DATE,\n","                                                                              STEPS_FORWARD,\n","                                                                              LAST_STEP_BACK)\n","\n","# X_train_3D = np.transpose(X_train_3D, (0, 2, 1))\n","# X_val_3D = np.transpose(X_val_3D, (0, 2, 1))\n","# X_test_3D = np.transpose(X_test_3D, (0, 2, 1))\n","\n","train_dataset = LoadDataset(X_3D=X_train_3D,\n","                            y_df=y_train_df)\n","train_dataloader = DataLoader(dataset=train_dataset,\n","                              batch_size=BATCH_SIZE,\n","                              shuffle=True)\n","\n","val_dataset = LoadDataset(X_3D=X_val_3D,\n","                          y_df=y_val_df)\n","val_dataloader = DataLoader(dataset=val_dataset,\n","                            batch_size=BATCH_SIZE)\n","\n","# val_dataset = LoadDataset(X_3D=X_test_3D,\n","#                           y_df=y_test_df)\n","# val_dataloader = DataLoader(dataset=val_dataset,\n","#                             batch_size=BATCH_SIZE)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HiHc8rNPkuEo","executionInfo":{"status":"ok","timestamp":1697215174515,"user_tz":-180,"elapsed":495,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}},"outputId":"a572c110-3c63-4191-a675-bad827bc0426"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(8473, 48, 8)\n","(8760, 48, 8)\n","(8497, 48, 8)\n"]}]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"22dObnhJNKH7"}},{"cell_type":"code","source":["EPOCHS = 2000\n","PATIENCE = 50\n","PATH = \"model.pth\"\n","\n","model = Encoder(D=X_train_3D.shape[2],\n","                max_len=LAST_STEP_BACK,\n","                d_k=16,\n","                d_model=64,\n","                n_heads=16,\n","                n_layers=3,\n","                output_units=24,\n","                dropout_prob=0).to(device)\n","\n","optimizer = torch.optim.Adam(params=model.parameters(),\n","                             lr=0.001,\n","                             weight_decay=0)\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.33, patience=20, verbose=True)"],"metadata":{"id":"YOrl-5I6Qmnh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -rf \"model.pth\"\n","start_time = datetime.now()\n","\n","model_results = train(model=model,\n","                      train_dataloader=train_dataloader,\n","                      val_dataloader=val_dataloader,\n","                      optimizer=optimizer,\n","                      epochs=EPOCHS,\n","                      patience=PATIENCE,\n","                      device=device,\n","                      scheduler=scheduler,\n","                      path=PATH)\n","\n","total_time = datetime.now() - start_time\n","print(f\"Total training time: {total_time.seconds}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b87d2795ed8741ee936b35517c74856f","30cd1aab593148a39a4a638c69c19d45","a0b1e302f4de4046920d1e77ed65caec","fe098c5f639d461db38f81f5b24d66a4","2258ef21397d47219619206432980bd0","a86c9e09624b4b3dab7cf427467994ec","0259d8443c924a4bb566a1f73e46649f","d58d241dfc6b4bbbaae5bd66bf519dbe","c0096800e2ac4476baf719a5ac253d41","208aabebfd6c4f1187bd17b11573259d","dfeecfc131c547718e3738f0716c612f"]},"id":"sI8MZ-vndz7_","executionInfo":{"status":"ok","timestamp":1697215357304,"user_tz":-180,"elapsed":175631,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}},"outputId":"bbc8ffbd-c311-4863-900e-01d83b42fa35"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2000 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b87d2795ed8741ee936b35517c74856f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 1/2000 | Loss: 0.0955 | Val loss: 23.1300 - *Checkpoint*\n","Epoch: 2/2000 | Loss: 0.0611 | Val loss: 20.9221 - *Checkpoint*\n","Epoch: 3/2000 | Loss: 0.0575 | Val loss: 16.9524 - *Checkpoint*\n","Epoch: 4/2000 | Loss: 0.0560 | Val loss: 17.1955\n","Epoch: 5/2000 | Loss: 0.0558 | Val loss: 16.3033 - *Checkpoint*\n","Epoch: 6/2000 | Loss: 0.0552 | Val loss: 15.8236 - *Checkpoint*\n","Epoch: 7/2000 | Loss: 0.0550 | Val loss: 15.6290 - *Checkpoint*\n","Epoch: 8/2000 | Loss: 0.0549 | Val loss: 15.2773 - *Checkpoint*\n","Epoch: 9/2000 | Loss: 0.0548 | Val loss: 15.2396 - *Checkpoint*\n","Epoch: 10/2000 | Loss: 0.0548 | Val loss: 15.2866\n","Epoch: 11/2000 | Loss: 0.0547 | Val loss: 15.3137\n","Epoch: 12/2000 | Loss: 0.0548 | Val loss: 15.3027\n","Epoch: 13/2000 | Loss: 0.0547 | Val loss: 15.2370 - *Checkpoint*\n","Epoch: 14/2000 | Loss: 0.0548 | Val loss: 15.2041 - *Checkpoint*\n","Epoch: 15/2000 | Loss: 0.0548 | Val loss: 15.1715 - *Checkpoint*\n","Epoch: 16/2000 | Loss: 0.0547 | Val loss: 15.1786\n","Epoch: 17/2000 | Loss: 0.0547 | Val loss: 15.1354 - *Checkpoint*\n","Epoch: 18/2000 | Loss: 0.0547 | Val loss: 15.1235 - *Checkpoint*\n","Epoch: 19/2000 | Loss: 0.0546 | Val loss: 15.1260\n","Epoch: 20/2000 | Loss: 0.0547 | Val loss: 15.1282\n","Epoch: 21/2000 | Loss: 0.0546 | Val loss: 15.0837 - *Checkpoint*\n","Epoch: 22/2000 | Loss: 0.0546 | Val loss: 15.0841\n","Epoch: 23/2000 | Loss: 0.0546 | Val loss: 15.0668 - *Checkpoint*\n","Epoch: 24/2000 | Loss: 0.0545 | Val loss: 15.0205 - *Checkpoint*\n","Epoch: 25/2000 | Loss: 0.0546 | Val loss: 15.0150 - *Checkpoint*\n","Epoch: 26/2000 | Loss: 0.0545 | Val loss: 14.9688 - *Checkpoint*\n","Epoch: 27/2000 | Loss: 0.0545 | Val loss: 14.9642 - *Checkpoint*\n","Epoch: 28/2000 | Loss: 0.0544 | Val loss: 14.8933 - *Checkpoint*\n","Epoch: 29/2000 | Loss: 0.0545 | Val loss: 14.8565 - *Checkpoint*\n","Epoch: 30/2000 | Loss: 0.0544 | Val loss: 14.8421 - *Checkpoint*\n","Epoch: 31/2000 | Loss: 0.0543 | Val loss: 14.8016 - *Checkpoint*\n","Epoch: 32/2000 | Loss: 0.0543 | Val loss: 14.6363 - *Checkpoint*\n","Epoch: 33/2000 | Loss: 0.0542 | Val loss: 14.6375\n","Epoch: 34/2000 | Loss: 0.0541 | Val loss: 14.5937 - *Checkpoint*\n","Epoch: 35/2000 | Loss: 0.0540 | Val loss: 14.5038 - *Checkpoint*\n","Epoch: 36/2000 | Loss: 0.0540 | Val loss: 14.4279 - *Checkpoint*\n","Epoch: 37/2000 | Loss: 0.0538 | Val loss: 14.3303 - *Checkpoint*\n","Epoch: 38/2000 | Loss: 0.0537 | Val loss: 13.9176 - *Checkpoint*\n","Epoch: 39/2000 | Loss: 0.0534 | Val loss: 13.5554 - *Checkpoint*\n","Epoch: 40/2000 | Loss: 0.0532 | Val loss: 13.2661 - *Checkpoint*\n","Epoch: 41/2000 | Loss: 0.0531 | Val loss: 13.9228\n","Epoch: 42/2000 | Loss: 0.0530 | Val loss: 12.9725 - *Checkpoint*\n","Epoch: 43/2000 | Loss: 0.0528 | Val loss: 12.2067 - *Checkpoint*\n","Epoch: 44/2000 | Loss: 0.0526 | Val loss: 12.1438 - *Checkpoint*\n","Epoch: 45/2000 | Loss: 0.0524 | Val loss: 11.8234 - *Checkpoint*\n","Epoch: 46/2000 | Loss: 0.0522 | Val loss: 11.2615 - *Checkpoint*\n","Epoch: 47/2000 | Loss: 0.0520 | Val loss: 11.4497\n","Epoch: 48/2000 | Loss: 0.0519 | Val loss: 10.6655 - *Checkpoint*\n","Epoch: 49/2000 | Loss: 0.0518 | Val loss: 10.7744\n","Epoch: 50/2000 | Loss: 0.0516 | Val loss: 10.0345 - *Checkpoint*\n","Epoch: 51/2000 | Loss: 0.0515 | Val loss: 9.7933 - *Checkpoint*\n","Epoch: 52/2000 | Loss: 0.0515 | Val loss: 9.9103\n","Epoch: 53/2000 | Loss: 0.0514 | Val loss: 9.5276 - *Checkpoint*\n","Epoch: 54/2000 | Loss: 0.0514 | Val loss: 9.6462\n","Epoch: 55/2000 | Loss: 0.0513 | Val loss: 9.2861 - *Checkpoint*\n","Epoch: 56/2000 | Loss: 0.0513 | Val loss: 9.2851 - *Checkpoint*\n","Epoch: 57/2000 | Loss: 0.0513 | Val loss: 9.2262 - *Checkpoint*\n","Epoch: 58/2000 | Loss: 0.0512 | Val loss: 9.0499 - *Checkpoint*\n","Epoch: 59/2000 | Loss: 0.0512 | Val loss: 9.0854\n","Epoch: 60/2000 | Loss: 0.0512 | Val loss: 9.0983\n","Epoch: 61/2000 | Loss: 0.0512 | Val loss: 8.7826 - *Checkpoint*\n","Epoch: 62/2000 | Loss: 0.0511 | Val loss: 8.8187\n","Epoch: 63/2000 | Loss: 0.0511 | Val loss: 8.6623 - *Checkpoint*\n","Epoch: 64/2000 | Loss: 0.0511 | Val loss: 8.7616\n","Epoch: 65/2000 | Loss: 0.0510 | Val loss: 8.5164 - *Checkpoint*\n","Epoch: 66/2000 | Loss: 0.0510 | Val loss: 8.4760 - *Checkpoint*\n","Epoch: 67/2000 | Loss: 0.0510 | Val loss: 8.4017 - *Checkpoint*\n","Epoch: 68/2000 | Loss: 0.0510 | Val loss: 8.5073\n","Epoch: 69/2000 | Loss: 0.0510 | Val loss: 8.2492 - *Checkpoint*\n","Epoch: 70/2000 | Loss: 0.0509 | Val loss: 8.2152 - *Checkpoint*\n","Epoch: 71/2000 | Loss: 0.0509 | Val loss: 8.2481\n","Epoch: 72/2000 | Loss: 0.0509 | Val loss: 8.0647 - *Checkpoint*\n","Epoch: 73/2000 | Loss: 0.0509 | Val loss: 8.0197 - *Checkpoint*\n","Epoch: 74/2000 | Loss: 0.0509 | Val loss: 8.0056 - *Checkpoint*\n","Epoch: 75/2000 | Loss: 0.0509 | Val loss: 8.0537\n","Epoch: 76/2000 | Loss: 0.0509 | Val loss: 8.0109\n","Epoch: 77/2000 | Loss: 0.0508 | Val loss: 8.0411\n","Epoch: 78/2000 | Loss: 0.0508 | Val loss: 7.9438 - *Checkpoint*\n","Epoch: 79/2000 | Loss: 0.0508 | Val loss: 7.8091 - *Checkpoint*\n","Epoch: 80/2000 | Loss: 0.0508 | Val loss: 7.7458 - *Checkpoint*\n","Epoch: 81/2000 | Loss: 0.0508 | Val loss: 7.7440 - *Checkpoint*\n","Epoch: 82/2000 | Loss: 0.0508 | Val loss: 7.9179\n","Epoch: 83/2000 | Loss: 0.0508 | Val loss: 7.6636 - *Checkpoint*\n","Epoch: 84/2000 | Loss: 0.0508 | Val loss: 8.0485\n","Epoch: 85/2000 | Loss: 0.0508 | Val loss: 7.7738\n","Epoch: 86/2000 | Loss: 0.0508 | Val loss: 7.8375\n","Epoch: 87/2000 | Loss: 0.0508 | Val loss: 7.8837\n","Epoch: 88/2000 | Loss: 0.0508 | Val loss: 7.5678 - *Checkpoint*\n","Epoch: 89/2000 | Loss: 0.0508 | Val loss: 7.5358 - *Checkpoint*\n","Epoch: 90/2000 | Loss: 0.0508 | Val loss: 7.6680\n","Epoch: 91/2000 | Loss: 0.0508 | Val loss: 7.4978 - *Checkpoint*\n","Epoch: 92/2000 | Loss: 0.0508 | Val loss: 7.5002\n","Epoch: 93/2000 | Loss: 0.0507 | Val loss: 7.8681\n","Epoch: 94/2000 | Loss: 0.0508 | Val loss: 7.6430\n","Epoch: 95/2000 | Loss: 0.0508 | Val loss: 7.6414\n","Epoch: 96/2000 | Loss: 0.0507 | Val loss: 7.4296 - *Checkpoint*\n","Epoch: 97/2000 | Loss: 0.0507 | Val loss: 7.4151 - *Checkpoint*\n","Epoch: 98/2000 | Loss: 0.0507 | Val loss: 7.4983\n","Epoch: 99/2000 | Loss: 0.0507 | Val loss: 7.3750 - *Checkpoint*\n","Epoch: 100/2000 | Loss: 0.0507 | Val loss: 7.3737 - *Checkpoint*\n","Epoch: 101/2000 | Loss: 0.0507 | Val loss: 7.3685 - *Checkpoint*\n","Epoch: 102/2000 | Loss: 0.0507 | Val loss: 7.3496 - *Checkpoint*\n","Epoch: 103/2000 | Loss: 0.0506 | Val loss: 7.3554\n","Epoch: 104/2000 | Loss: 0.0507 | Val loss: 7.2999 - *Checkpoint*\n","Epoch: 105/2000 | Loss: 0.0507 | Val loss: 7.3419\n","Epoch: 106/2000 | Loss: 0.0507 | Val loss: 7.2902 - *Checkpoint*\n","Epoch: 107/2000 | Loss: 0.0507 | Val loss: 7.3008\n","Epoch: 108/2000 | Loss: 0.0506 | Val loss: 7.6253\n","Epoch: 109/2000 | Loss: 0.0507 | Val loss: 7.2495 - *Checkpoint*\n","Epoch: 110/2000 | Loss: 0.0506 | Val loss: 7.2445 - *Checkpoint*\n","Epoch: 111/2000 | Loss: 0.0506 | Val loss: 7.4522\n","Epoch: 112/2000 | Loss: 0.0506 | Val loss: 7.4956\n","Epoch: 113/2000 | Loss: 0.0506 | Val loss: 7.6595\n","Epoch: 114/2000 | Loss: 0.0506 | Val loss: 7.2001 - *Checkpoint*\n","Epoch: 115/2000 | Loss: 0.0506 | Val loss: 7.1993 - *Checkpoint*\n","Epoch: 116/2000 | Loss: 0.0506 | Val loss: 7.4544\n","Epoch: 117/2000 | Loss: 0.0506 | Val loss: 7.3432\n","Epoch: 118/2000 | Loss: 0.0506 | Val loss: 7.1510 - *Checkpoint*\n","Epoch: 119/2000 | Loss: 0.0507 | Val loss: 7.9506\n","Epoch: 120/2000 | Loss: 0.0507 | Val loss: 7.7994\n","Epoch: 121/2000 | Loss: 0.0507 | Val loss: 7.1085 - *Checkpoint*\n","Epoch: 122/2000 | Loss: 0.0506 | Val loss: 7.0823 - *Checkpoint*\n","Epoch: 123/2000 | Loss: 0.0506 | Val loss: 7.6505\n","Epoch: 124/2000 | Loss: 0.0506 | Val loss: 7.0491 - *Checkpoint*\n","Epoch: 125/2000 | Loss: 0.0506 | Val loss: 7.0316 - *Checkpoint*\n","Epoch: 126/2000 | Loss: 0.0506 | Val loss: 7.1092\n","Epoch: 127/2000 | Loss: 0.0506 | Val loss: 7.4970\n","Epoch: 128/2000 | Loss: 0.0506 | Val loss: 7.0556\n","Epoch: 129/2000 | Loss: 0.0505 | Val loss: 6.9789 - *Checkpoint*\n","Epoch: 130/2000 | Loss: 0.0505 | Val loss: 7.3842\n","Epoch: 131/2000 | Loss: 0.0506 | Val loss: 7.2693\n","Epoch: 132/2000 | Loss: 0.0506 | Val loss: 7.0616\n","Epoch: 133/2000 | Loss: 0.0505 | Val loss: 6.9451 - *Checkpoint*\n","Epoch: 134/2000 | Loss: 0.0505 | Val loss: 6.9273 - *Checkpoint*\n","Epoch: 135/2000 | Loss: 0.0505 | Val loss: 6.9179 - *Checkpoint*\n","Epoch: 136/2000 | Loss: 0.0505 | Val loss: 6.9063 - *Checkpoint*\n","Epoch: 137/2000 | Loss: 0.0506 | Val loss: 6.9908\n","Epoch: 138/2000 | Loss: 0.0507 | Val loss: 7.2184\n","Epoch: 139/2000 | Loss: 0.0506 | Val loss: 6.8731 - *Checkpoint*\n","Epoch: 140/2000 | Loss: 0.0506 | Val loss: 7.0782\n","Epoch: 141/2000 | Loss: 0.0506 | Val loss: 6.9691\n","Epoch: 142/2000 | Loss: 0.0506 | Val loss: 7.6141\n","Epoch: 143/2000 | Loss: 0.0505 | Val loss: 7.1807\n","Epoch: 144/2000 | Loss: 0.0505 | Val loss: 6.8970\n","Epoch: 145/2000 | Loss: 0.0506 | Val loss: 6.7715 - *Checkpoint*\n","Epoch: 146/2000 | Loss: 0.0505 | Val loss: 7.7057\n","Epoch: 147/2000 | Loss: 0.0505 | Val loss: 6.8873\n","Epoch: 148/2000 | Loss: 0.0505 | Val loss: 6.8154\n","Epoch: 149/2000 | Loss: 0.0505 | Val loss: 6.6877 - *Checkpoint*\n","Epoch: 150/2000 | Loss: 0.0505 | Val loss: 6.8143\n","Epoch: 151/2000 | Loss: 0.0505 | Val loss: 6.6652 - *Checkpoint*\n","Epoch: 152/2000 | Loss: 0.0505 | Val loss: 6.6356 - *Checkpoint*\n","Epoch: 153/2000 | Loss: 0.0505 | Val loss: 6.6929\n","Epoch: 154/2000 | Loss: 0.0505 | Val loss: 6.7399\n","Epoch: 155/2000 | Loss: 0.0505 | Val loss: 7.0955\n","Epoch: 156/2000 | Loss: 0.0506 | Val loss: 6.7831\n","Epoch: 157/2000 | Loss: 0.0505 | Val loss: 7.5572\n","Epoch: 158/2000 | Loss: 0.0505 | Val loss: 8.0125\n","Epoch: 159/2000 | Loss: 0.0505 | Val loss: 6.5180 - *Checkpoint*\n","Epoch: 160/2000 | Loss: 0.0505 | Val loss: 6.6012\n","Epoch: 161/2000 | Loss: 0.0504 | Val loss: 7.1476\n","Epoch: 162/2000 | Loss: 0.0505 | Val loss: 6.8644\n","Epoch: 163/2000 | Loss: 0.0504 | Val loss: 6.6711\n","Epoch: 164/2000 | Loss: 0.0504 | Val loss: 7.0359\n","Epoch: 165/2000 | Loss: 0.0504 | Val loss: 6.7105\n","Epoch: 166/2000 | Loss: 0.0504 | Val loss: 7.0380\n","Epoch: 167/2000 | Loss: 0.0504 | Val loss: 7.5168\n","Epoch: 168/2000 | Loss: 0.0504 | Val loss: 6.4852 - *Checkpoint*\n","Epoch: 169/2000 | Loss: 0.0505 | Val loss: 7.8639\n","Epoch: 170/2000 | Loss: 0.0504 | Val loss: 6.9439\n","Epoch: 171/2000 | Loss: 0.0504 | Val loss: 6.7486\n","Epoch: 172/2000 | Loss: 0.0504 | Val loss: 7.4485\n","Epoch: 173/2000 | Loss: 0.0507 | Val loss: 6.3738 - *Checkpoint*\n","Epoch: 174/2000 | Loss: 0.0505 | Val loss: 7.4499\n","Epoch: 175/2000 | Loss: 0.0504 | Val loss: 6.4693\n","Epoch: 176/2000 | Loss: 0.0504 | Val loss: 6.9258\n","Epoch: 177/2000 | Loss: 0.0504 | Val loss: 6.9807\n","Epoch: 178/2000 | Loss: 0.0504 | Val loss: 6.4547\n","Epoch: 179/2000 | Loss: 0.0504 | Val loss: 6.8002\n","Epoch: 180/2000 | Loss: 0.0504 | Val loss: 7.9321\n","Epoch: 181/2000 | Loss: 0.0505 | Val loss: 6.7679\n","Epoch: 182/2000 | Loss: 0.0503 | Val loss: 6.4510\n","Epoch: 183/2000 | Loss: 0.0503 | Val loss: 6.5210\n","Epoch: 184/2000 | Loss: 0.0503 | Val loss: 6.4524\n","Epoch: 185/2000 | Loss: 0.0503 | Val loss: 6.4381\n","Epoch: 186/2000 | Loss: 0.0509 | Val loss: 6.3392 - *Checkpoint*\n","Epoch: 187/2000 | Loss: 0.0506 | Val loss: 8.6724\n","Epoch: 188/2000 | Loss: 0.0505 | Val loss: 6.4342\n","Epoch: 189/2000 | Loss: 0.0504 | Val loss: 6.9631\n","Epoch: 190/2000 | Loss: 0.0504 | Val loss: 6.2489 - *Checkpoint*\n","Epoch: 191/2000 | Loss: 0.0504 | Val loss: 7.0948\n","Epoch: 192/2000 | Loss: 0.0503 | Val loss: 6.1826 - *Checkpoint*\n","Epoch: 193/2000 | Loss: 0.0503 | Val loss: 6.4680\n","Epoch: 194/2000 | Loss: 0.0503 | Val loss: 6.6140\n","Epoch: 195/2000 | Loss: 0.0503 | Val loss: 6.2774\n","Epoch: 196/2000 | Loss: 0.0503 | Val loss: 6.8132\n","Epoch: 197/2000 | Loss: 0.0503 | Val loss: 6.5071\n","Epoch: 198/2000 | Loss: 0.0503 | Val loss: 6.3817\n","Epoch: 199/2000 | Loss: 0.0503 | Val loss: 6.1814 - *Checkpoint*\n","Epoch: 200/2000 | Loss: 0.0503 | Val loss: 6.2514\n","Epoch: 201/2000 | Loss: 0.0504 | Val loss: 6.2508\n","Epoch: 202/2000 | Loss: 0.0504 | Val loss: 6.1592 - *Checkpoint*\n","Epoch: 203/2000 | Loss: 0.0504 | Val loss: 6.2035\n","Epoch: 204/2000 | Loss: 0.0503 | Val loss: 6.3646\n","Epoch: 205/2000 | Loss: 0.0503 | Val loss: 6.1342 - *Checkpoint*\n","Epoch: 206/2000 | Loss: 0.0503 | Val loss: 6.3453\n","Epoch: 207/2000 | Loss: 0.0504 | Val loss: 6.1647\n","Epoch: 208/2000 | Loss: 0.0503 | Val loss: 7.8748\n","Epoch: 209/2000 | Loss: 0.0503 | Val loss: 7.1571\n","Epoch: 210/2000 | Loss: 0.0503 | Val loss: 6.2137\n","Epoch: 211/2000 | Loss: 0.0503 | Val loss: 6.4995\n","Epoch: 212/2000 | Loss: 0.0503 | Val loss: 6.7497\n","Epoch: 213/2000 | Loss: 0.0502 | Val loss: 7.7009\n","Epoch: 214/2000 | Loss: 0.0503 | Val loss: 7.1751\n","Epoch: 215/2000 | Loss: 0.0503 | Val loss: 8.1914\n","Epoch: 216/2000 | Loss: 0.0504 | Val loss: 6.2885\n","Epoch: 217/2000 | Loss: 0.0503 | Val loss: 6.3577\n","Epoch: 218/2000 | Loss: 0.0503 | Val loss: 6.1136 - *Checkpoint*\n","Epoch: 219/2000 | Loss: 0.0503 | Val loss: 5.9713 - *Checkpoint*\n","Epoch: 220/2000 | Loss: 0.0503 | Val loss: 6.0074\n","Epoch: 221/2000 | Loss: 0.0503 | Val loss: 6.4385\n","Epoch: 222/2000 | Loss: 0.0502 | Val loss: 6.4783\n","Epoch: 223/2000 | Loss: 0.0502 | Val loss: 7.1599\n","Epoch: 224/2000 | Loss: 0.0503 | Val loss: 7.3218\n","Epoch: 225/2000 | Loss: 0.0506 | Val loss: 6.1376\n","Epoch: 226/2000 | Loss: 0.0503 | Val loss: 6.6884\n","Epoch: 227/2000 | Loss: 0.0503 | Val loss: 5.9653 - *Checkpoint*\n","Epoch: 228/2000 | Loss: 0.0503 | Val loss: 6.3415\n","Epoch: 229/2000 | Loss: 0.0502 | Val loss: 7.5537\n","Epoch: 230/2000 | Loss: 0.0503 | Val loss: 7.1850\n","Epoch: 231/2000 | Loss: 0.0502 | Val loss: 5.9241 - *Checkpoint*\n","Epoch: 232/2000 | Loss: 0.0502 | Val loss: 5.9873\n","Epoch: 233/2000 | Loss: 0.0502 | Val loss: 6.0167\n","Epoch: 234/2000 | Loss: 0.0502 | Val loss: 6.0355\n","Epoch: 235/2000 | Loss: 0.0502 | Val loss: 6.0763\n","Epoch: 236/2000 | Loss: 0.0502 | Val loss: 6.0107\n","Epoch: 237/2000 | Loss: 0.0503 | Val loss: 7.1730\n","Epoch: 238/2000 | Loss: 0.0502 | Val loss: 6.2157\n","Epoch: 239/2000 | Loss: 0.0502 | Val loss: 6.3025\n","Epoch: 240/2000 | Loss: 0.0502 | Val loss: 6.9499\n","Epoch: 241/2000 | Loss: 0.0502 | Val loss: 6.3988\n","Epoch: 242/2000 | Loss: 0.0502 | Val loss: 6.1539\n","Epoch: 243/2000 | Loss: 0.0503 | Val loss: 5.9793\n","Epoch: 244/2000 | Loss: 0.0502 | Val loss: 6.2379\n","Epoch: 245/2000 | Loss: 0.0502 | Val loss: 6.3566\n","Epoch: 246/2000 | Loss: 0.0502 | Val loss: 6.2271\n","Epoch: 247/2000 | Loss: 0.0502 | Val loss: 7.8439\n","Epoch: 248/2000 | Loss: 0.0503 | Val loss: 6.0556\n","Epoch: 249/2000 | Loss: 0.0502 | Val loss: 6.6181\n","Epoch: 250/2000 | Loss: 0.0502 | Val loss: 6.4219\n","Epoch: 251/2000 | Loss: 0.0502 | Val loss: 8.8213\n","Epoch 00252: reducing learning rate of group 0 to 3.3000e-04.\n","Epoch: 252/2000 | Loss: 0.0514 | Val loss: 14.0182\n","Epoch: 253/2000 | Loss: 0.0512 | Val loss: 8.6117\n","Epoch: 254/2000 | Loss: 0.0509 | Val loss: 10.6382\n","Epoch: 255/2000 | Loss: 0.0508 | Val loss: 7.3174\n","Epoch: 256/2000 | Loss: 0.0506 | Val loss: 8.7617\n","Epoch: 257/2000 | Loss: 0.0504 | Val loss: 6.2277\n","Epoch: 258/2000 | Loss: 0.0503 | Val loss: 5.9616\n","Epoch: 259/2000 | Loss: 0.0502 | Val loss: 7.1361\n","Epoch: 260/2000 | Loss: 0.0502 | Val loss: 6.5981\n","Epoch: 261/2000 | Loss: 0.0502 | Val loss: 5.8690 - *Checkpoint*\n","Epoch: 262/2000 | Loss: 0.0502 | Val loss: 5.9282\n","Epoch: 263/2000 | Loss: 0.0502 | Val loss: 6.3377\n","Epoch: 264/2000 | Loss: 0.0502 | Val loss: 6.6793\n","Epoch: 265/2000 | Loss: 0.0502 | Val loss: 6.1607\n","Epoch: 266/2000 | Loss: 0.0502 | Val loss: 5.9747\n","Epoch: 267/2000 | Loss: 0.0502 | Val loss: 6.0617\n","Epoch: 268/2000 | Loss: 0.0502 | Val loss: 6.4236\n","Epoch: 269/2000 | Loss: 0.0502 | Val loss: 6.4414\n","Epoch: 270/2000 | Loss: 0.0501 | Val loss: 6.4247\n","Epoch: 271/2000 | Loss: 0.0501 | Val loss: 6.3023\n","Epoch: 272/2000 | Loss: 0.0501 | Val loss: 6.3694\n","Epoch: 273/2000 | Loss: 0.0502 | Val loss: 6.3198\n","Epoch: 274/2000 | Loss: 0.0501 | Val loss: 6.1868\n","Epoch: 275/2000 | Loss: 0.0501 | Val loss: 6.2043\n","Epoch: 276/2000 | Loss: 0.0501 | Val loss: 6.4130\n","Epoch: 277/2000 | Loss: 0.0501 | Val loss: 6.3542\n","Epoch: 278/2000 | Loss: 0.0501 | Val loss: 6.4682\n","Epoch: 279/2000 | Loss: 0.0501 | Val loss: 6.5275\n","Epoch: 280/2000 | Loss: 0.0501 | Val loss: 6.4557\n","Epoch: 281/2000 | Loss: 0.0501 | Val loss: 6.3748\n","Epoch 00282: reducing learning rate of group 0 to 1.0890e-04.\n","Epoch: 282/2000 | Loss: 0.0501 | Val loss: 6.5540\n","Epoch: 283/2000 | Loss: 0.0501 | Val loss: 6.4358\n","Epoch: 284/2000 | Loss: 0.0501 | Val loss: 6.5050\n","Epoch: 285/2000 | Loss: 0.0501 | Val loss: 6.4573\n","Epoch: 286/2000 | Loss: 0.0501 | Val loss: 6.5420\n","Epoch: 287/2000 | Loss: 0.0501 | Val loss: 6.4676\n","Epoch: 288/2000 | Loss: 0.0501 | Val loss: 6.5144\n","Epoch: 289/2000 | Loss: 0.0501 | Val loss: 6.4595\n","Epoch: 290/2000 | Loss: 0.0501 | Val loss: 6.5546\n","Epoch: 291/2000 | Loss: 0.0501 | Val loss: 6.4030\n","Epoch: 292/2000 | Loss: 0.0501 | Val loss: 6.6352\n","Epoch: 293/2000 | Loss: 0.0501 | Val loss: 6.3866\n","Epoch: 294/2000 | Loss: 0.0501 | Val loss: 6.6538\n","Epoch: 295/2000 | Loss: 0.0501 | Val loss: 6.4025\n","Epoch: 296/2000 | Loss: 0.0501 | Val loss: 6.6311\n","Epoch: 297/2000 | Loss: 0.0501 | Val loss: 6.4143\n","Epoch: 298/2000 | Loss: 0.0501 | Val loss: 6.6574\n","Epoch: 299/2000 | Loss: 0.0501 | Val loss: 6.3958\n","Epoch: 300/2000 | Loss: 0.0501 | Val loss: 6.5958\n","Epoch: 301/2000 | Loss: 0.0501 | Val loss: 6.4961\n","Epoch: 302/2000 | Loss: 0.0501 | Val loss: 6.5793\n","Epoch 00303: reducing learning rate of group 0 to 3.5937e-05.\n","Epoch: 303/2000 | Loss: 0.0501 | Val loss: 6.5118\n","Epoch: 304/2000 | Loss: 0.0501 | Val loss: 6.5498\n","Epoch: 305/2000 | Loss: 0.0501 | Val loss: 6.5521\n","Epoch: 306/2000 | Loss: 0.0501 | Val loss: 6.5880\n","Epoch: 307/2000 | Loss: 0.0501 | Val loss: 6.5150\n","Epoch: 308/2000 | Loss: 0.0501 | Val loss: 6.5582\n","Epoch: 309/2000 | Loss: 0.0501 | Val loss: 6.6103\n","Epoch: 310/2000 | Loss: 0.0501 | Val loss: 6.5162\n","Epoch: 311/2000 | Loss: 0.0501 | Val loss: 6.4774\n","\n","Early stopping applied at epoch 311.\n","Total training time: 175\n"]}]},{"cell_type":"code","source":["# print(model.a, model.b, model.c)"],"metadata":{"id":"7KIqdyklb0KU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference + results"],"metadata":{"id":"U1I3fV-ubspN"}},{"cell_type":"code","source":["checkpoint = torch.load(\"model.pth\")\n","model.load_state_dict(checkpoint['model'])\n","optimizer.load_state_dict(checkpoint['optimizer'])\n","# model.to(\"cpu\")\n","model.eval()\n","with torch.inference_mode():\n","  test_preds_scaled = model(torch.tensor(X_test_3D, dtype=torch.float32, device=device))\n","test_preds_scaled = test_preds_scaled.to('cpu').squeeze().numpy()\n","\n","test_preds_df_scaled = pd.DataFrame(test_preds_scaled, columns=np.arange(1, STEPS_FORWARD+1), index=y_test_df.index)\n","test_preds_df = pd.DataFrame(columns=np.arange(1, STEPS_FORWARD+1), index=test_preds_df_scaled.index)\n","for i, col in enumerate(test_preds_df_scaled.columns):\n","  test_preds_df[i+1] = scaler.data_min_[TARGET_POS] + test_preds_df_scaled[col].to_numpy() * (scaler.data_max_[TARGET_POS] - scaler.data_min_[TARGET_POS])\n","\n","real_df = pd.DataFrame(columns=np.arange(1, STEPS_FORWARD+1), index=test_preds_df.index)\n","for i, col in enumerate(y_test_df.columns):\n","  real_df[i+1] = scaler.data_min_[TARGET_POS] + y_test_df[col].to_numpy() * (scaler.data_max_[TARGET_POS] - scaler.data_min_[TARGET_POS])"],"metadata":{"id":"PFvF3h-7cXg3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mape_list = list()\n","step_results_dict = {}\n","for step in range(1, STEPS_FORWARD + 1):\n","  step_index = real_df.index + pd.to_timedelta(SKIP_STEPS_FORWARD + step, 'h')\n","  step_results_df = pd.DataFrame(\n","      {\n","          \"real\": real_df.loc[:, step].to_numpy(),\n","          \"predictions\": test_preds_df.loc[:, step].to_numpy()\n","      },\n","      index=step_index\n","  )\n","  step_results_df['abs_error'] = abs(step_results_df['real'] - step_results_df['predictions'])\n","  step_results_df['ape'] = np.where(step_results_df['real'] == 0, np.NaN, 100 * step_results_df['abs_error']/step_results_df['real'])\n","  step_mape = step_results_df['ape'].mean()\n","  mape_list.append(step_mape)\n","  print(f\"Step {step} -> MAPE = {step_mape}\")\n","\n","  step_results_dict[step] = step_results_df\n","mape = np.array(mape_list).mean()\n","print(f\"\\nMAPE = {mape}\")"],"metadata":{"id":"EJSskMCYKBb5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697215358937,"user_tz":-180,"elapsed":6,"user":{"displayName":"Achilleas Palaskos","userId":"04608072963754583562"}},"outputId":"cde92454-7b9f-45e0-d689-176155368bc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Step 1 -> MAPE = 6.338359021428407\n","Step 2 -> MAPE = 6.598430493774094\n","Step 3 -> MAPE = 7.0015727889691535\n","Step 4 -> MAPE = 7.056319747862323\n","Step 5 -> MAPE = 7.221356026957652\n","Step 6 -> MAPE = 7.2484299869286355\n","Step 7 -> MAPE = 6.772071625608724\n","Step 8 -> MAPE = 6.74384771108786\n","Step 9 -> MAPE = 6.575050010331082\n","Step 10 -> MAPE = 6.690373168505351\n","Step 11 -> MAPE = 6.570262681127452\n","Step 12 -> MAPE = 6.853364710619025\n","Step 13 -> MAPE = 6.649800424786464\n","Step 14 -> MAPE = 6.79072418450538\n","Step 15 -> MAPE = 6.782684200862485\n","Step 16 -> MAPE = 6.4834693463184525\n","Step 17 -> MAPE = 7.086297897830933\n","Step 18 -> MAPE = 6.958019544765056\n","Step 19 -> MAPE = 6.690238410067856\n","Step 20 -> MAPE = 6.452968206653064\n","Step 21 -> MAPE = 6.46728460865005\n","Step 22 -> MAPE = 6.611000009341176\n","Step 23 -> MAPE = 6.576734019047573\n","Step 24 -> MAPE = 6.3914526353811985\n","\n","MAPE = 6.733754644225393\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"FpB44T5lUsoQ"},"execution_count":null,"outputs":[]}]}